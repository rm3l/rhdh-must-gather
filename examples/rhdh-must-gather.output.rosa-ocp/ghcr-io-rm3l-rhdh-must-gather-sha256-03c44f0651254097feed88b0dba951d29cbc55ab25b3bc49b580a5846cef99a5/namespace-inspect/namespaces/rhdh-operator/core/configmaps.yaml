---
apiVersion: v1
items:
- apiVersion: v1
  data:
    ca.crt: [REDACTED]
      -----BEGIN CERTIFICATE-----
      MIIDPDCCAiSgAwIBAgIIN4j+ygo0oE8wDQYJKoZIhvcNAQELBQAwJjESMBAGA1UE
      CxMJb3BlbnNoaWZ0MRAwDgYDVQQDEwdyb290LWNhMB4XDTI1MTAyMzA3MDk0NVoX
      DTM1MTAyMTA3MDk0NVowJjESMBAGA1UECxMJb3BlbnNoaWZ0MRAwDgYDVQQDEwdy
      b290LWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmNHshIFiKu+g
      l5biuVj5bQ1G45y2dqpdEF7VOxTkCn03oMa62eu2JdCVxkDBjIOwx3JMN3U0hGmT
      MPcSWM53A9bHSbEnl8e13ycaJI1QnqAsueBVaB2DAv3l6y+VJa1ZiyteQ7WTV/Hi
      LUuxXXX87E3BpuIBIPnQPFT2KHZNZTuOzzCxABxACOIbXL7wq+BxgQBlzXHWhsa6
      mAgWv34fwbXNMWk/LxnVquoj+2T38c+QpjqXgEHH4hCN+K4u2Egg9mZcscRk2CAY
      4pnVVhnBrkQdiveEGFBk9TIzchxSBOiEa8ryq/HRjE4F3D8e1//9YceuMu24ZhNI
      AoOQJvF+TQIDAQABo24wbDAOBgNVHQ8BAf8EBAMCAqQwDwYDVR0TAQH/BAUwAwEB
      /zBJBgNVHQ4EQgRAFCReZnPyLv3cA/Mq1cqYKhXNxIDUOVaoVEYCrvaY3lnYJB8l
      qDQ6rXKjoYFNok82bcPj45Q7lsIg9J3fzV4EBDANBgkqhkiG9w0BAQsFAAOCAQEA
      H7ccPhnx5JW4AzFNmmmwCY4aqJ68HYnbynahQvr/tiuz/OJJhkShYkN5QeQqdKG6
      JlFHPvTjYTQkI+c3GEatTB+SrN7ea2WZN67BdhSsTnypm9fBAVUyPpHZw+uz37+1
      2yPpeVEhwkZmjQ6nxzBkVrKYBgG2ZKuwqEiTIoRyeuds5GVLrzo9GPZc0PhmqETj
      neRoxkmjf9F9S0mK02Qihcz9NwgZU38wIYoDCB5YEsL466DhJ+Zsx4hovVzWLDYB
      9f9JwuvOo1RZnNPDfWLzgEjq/0HmI6u/Uf9cdu7d0tBjZTccVkcLzdddqdHVnThP
      M0Bd/oTm5/PRR8uwNc8CmQ==
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIFnDCCBISgAwIBAgISBa/Z1DANfGaBT5kcJXKcn5bbMA0GCSqGSIb3DQEBCwUA
      MDMxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1MZXQncyBFbmNyeXB0MQwwCgYDVQQD
      EwNSMTIwHhcNMjUxMDIzMDYxMTA3WhcNMjYwMTIxMDYxMTA2WjBAMT4wPAYDVQQD
      DDUqLmFwcHMucm9zYS5wZGdiYS1hY3FtMy0zZW0uNDJ3ei5wMy5vcGVuc2hpZnRh
      cHBzLmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMVOYT7NGtmL
      //MUfgYh1/2wpoQ7oN39Lwf0VyMKF89MzealePvLZKycuRD8lMHuPlNDeslZAATk
      u1G9sZtzeQ5nMYRFrM8isjIKqdE/pqWfWGHzOImytrCO4rTRPcu9Ex90GtCtmWk3
      ffXSYFC+ysTXGvLvoxymBzYpOd4vsss8F09xEluEFuhM0tMZFmX3qWqQj1119jNf
      MSsLcjqqCBu6m45+Z2Fv/fvgV2GIM3SObN9aqYJldFLmvLawKaB7yXu7gIyGpxn5
      ZlrsQv/vQBcIvsp9ksso01K4jJ6FXwDi6sDuGFF1T3og7G90mr4q8z+DvQkR5Z6F
      IMtBGhdYO7sCAwEAAaOCApswggKXMA4GA1UdDwEB/wQEAwIFoDAdBgNVHSUEFjAU
      BggrBgEFBQcDAQYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADAdBgNVHQ4EFgQUg814
      qcYIzfkHhdTsHhyLtdJGsjAwHwYDVR0jBBgwFoAUALUp8i2ObzHom0yteD763OkM
      0dIwMwYIKwYBBQUHAQEEJzAlMCMGCCsGAQUFBzAChhdodHRwOi8vcjEyLmkubGVu
      Y3Iub3JnLzCBlQYDVR0RBIGNMIGKglEqLjJtM29hN2JsaWoyZGExNjE0NjJoMGsx
      OGFqZGc4bTR1LnJvc2EucGRnYmEtYWNxbTMtM2VtLjQyd3oucDMub3BlbnNoaWZ0
      YXBwcy5jb22CNSouYXBwcy5yb3NhLnBkZ2JhLWFjcW0zLTNlbS40Mnd6LnAzLm9w
      ZW5zaGlmdGFwcHMuY29tMBMGA1UdIAQMMAowCAYGZ4EMAQIBMC4GA1UdHwQnMCUw
      I6AhoB+GHWh0dHA6Ly9yMTIuYy5sZW5jci5vcmcvOTIuY3JsMIIBBAYKKwYBBAHW
      eQIEAgSB9QSB8gDwAHYADleUvPOuqT4zGyyZB7P3kN+bwj1xMiXdIaklrGHFTiEA
      AAGaD+bZvAAABAMARzBFAiEA9SFaF8Z1Jj/BomHa8hk2XOmb1WNDqM4rnheC5Ot8
      cCsCIBZkywE80DdixP+U3Zno3FX+0zIeEjdL6kYU7Tsbrv7FAHYAyzj3FYl8hKFE
      X1vB3fvJbvKaWc1HCmkFhbDLFMMUWOcAAAGaD+bhlwAABAMARzBFAiAef2/prz1c
      Hg0c+YqF14/Jng0UBtVyLLn7I6tJnIO1fwIhAKuL5YsTH4+T5xxx8niJgTFWuAZD
      WJuqGi51bEAaX7TIMA0GCSqGSIb3DQEBCwUAA4IBAQCCKr7pWNzZ5EpwwIcft9GU
      AZqksrGSzGXS9myd6v3Ul+2pfMug/faAPYMKIqXqN7uyEuei/FN44EdEzBZbQ2X/
      /Mtjys/iIJpiATJGPKmUkoO3O+zN7GH+ueQiS+3AMrCfhsimoXxa7g5OcufyP+XL
      HidGxIhkRjX0tCXgye6bE/a0kSUPk0/W0y5J4Fz5ih5b/gCjxw0MhqeTAJCGfd/T
      HJoLaTlWvwgef7Vbpqtr/INg7lkYFDh/w9WhCbf8GsaeajyCPEyJAVy2jxapTbo5
      88DY8K677KWK5nEHgxrWsEvaGzPwl7ZxxREEPGbohVau3BNHQbvSy+OacyJrQrJG
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIFBjCCAu6gAwIBAgIRAMISMktwqbSRcdxA9+KFJjwwDQYJKoZIhvcNAQELBQAw
      TzELMAkGA1UEBhMCVVMxKTAnBgNVBAoTIEludGVybmV0IFNlY3VyaXR5IFJlc2Vh
      cmNoIEdyb3VwMRUwEwYDVQQDEwxJU1JHIFJvb3QgWDEwHhcNMjQwMzEzMDAwMDAw
      WhcNMjcwMzEyMjM1OTU5WjAzMQswCQYDVQQGEwJVUzEWMBQGA1UEChMNTGV0J3Mg
      RW5jcnlwdDEMMAoGA1UEAxMDUjEyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIB
      CgKCAQEA2pgodK2+lP474B7i5Ut1qywSf+2nAzJ+Npfs6DGPpRONC5kuHs0BUT1M
      5ShuCVUxqqUiXXL0LQfCTUA83wEjuXg39RplMjTmhnGdBO+ECFu9AhqZ66YBAJpz
      kG2Pogeg0JfT2kVhgTU9FPnEwF9q3AuWGrCf4yrqvSrWmMebcas7dA8827JgvlpL
      Thjp2ypzXIlhZZ7+7Tymy05v5J75AEaz/xlNKmOzjmbGGIVwx1Blbzt05UiDDwhY
      XS0jnV6j/ujbAKHS9OMZTfLuevYnnuXNnC2i8n+cF63vEzc50bTILEHWhsDp7CH4
      WRt/uTp8n1wBnWIEwii9Cq08yhDsGwIDAQABo4H4MIH1MA4GA1UdDwEB/wQEAwIB
      hjAdBgNVHSUEFjAUBggrBgEFBQcDAgYIKwYBBQUHAwEwEgYDVR0TAQH/BAgwBgEB
      /wIBADAdBgNVHQ4EFgQUALUp8i2ObzHom0yteD763OkM0dIwHwYDVR0jBBgwFoAU
      ebRZ5nu25eQBc4AIiMgaWPbpm24wMgYIKwYBBQUHAQEEJjAkMCIGCCsGAQUFBzAC
      hhZodHRwOi8veDEuaS5sZW5jci5vcmcvMBMGA1UdIAQMMAowCAYGZ4EMAQIBMCcG
      A1UdHwQgMB4wHKAaoBiGFmh0dHA6Ly94MS5jLmxlbmNyLm9yZy8wDQYJKoZIhvcN
      AQELBQADggIBAI910AnPanZIZTKS3rVEyIV29BWEjAK/duuz8eL5boSoVpHhkkv3
      4eoAeEiPdZLj5EZ7G2ArIK+gzhTlRQ1q4FKGpPPaFBSpqV/xbUb5UlAXQOnkHn3m
      FVj+qYv87/WeY+Bm4sN3Ox8BhyaU7UAQ3LeZ7N1X01xxQe4wIAAE3JVLUCiHmZL+
      qoCUtgYIFPgcg350QMUIWgxPXNGEncT921ne7nluI02V8pLUmClqXOsCwULw+PVO
      ZCB7qOMxxMBoCUeL2Ll4oMpOSr5pJCpLN3tRA2s6P1KLs9TSrVhOk+7LX28NMUlI
      usQ/nxLJID0RhAeFtPjyOCOscQBA53+NRjSCak7P4A5jX7ppmkcJECL+S0i3kXVU
      y5Me5BbrU8973jZNv/ax6+ZK6TM8jWmimL6of6OrX7ZU6E2WqazzsFrLG3o2kySb
      zlhSgJ81Cl4tv3SbYiYXnJExKQvzf83DYotox3f0fwv7xln1A2ZLplCb0O+l/AK0
      YE0DS2FPxSAHi0iwMfW2nNHJrXcY3LLHD77gRgje4Eveubi2xxa+Nmk/hmhLdIET
      iVDFanoCrMVIpQ59XWHkzdFmoHXHBV7oibVjGSO7ULSQ7MJ1Nz51phuDJSgAIU7A
      0zrLnOrAj/dfrlEWRhCvAgbuwLZX1A2sjNjXoPOHbsPiy+lO1KF8/XY7
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    annotations:
      kubernetes.io/description: Contains a CA bundle that can be used to verify the
        kube-apiserver when using internal endpoints such as the internal service
        IP or kubernetes.default.svc. No other usage is guaranteed across distributions
        of Kubernetes clusters.
    creationTimestamp: "2025-10-23T11:08:48Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          .: {}
          f:ca.crt: {}
        f:metadata:
          f:annotations:
            .: {}
            f:kubernetes.io/description: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-10-23T11:08:48Z"
    name: kube-root-ca.crt
    namespace: rhdh-operator
    resourceVersion: "39083"
    uid: 1c7aa2fb-2d3f-4302-bc3d-40053b81a29a
- apiVersion: v1
  data:
    service-ca.crt: [REDACTED]
      -----BEGIN CERTIFICATE-----
      MIIDUTCCAjmgAwIBAgIIWA9iBjHdz5QwDQYJKoZIhvcNAQELBQAwNjE0MDIGA1UE
      Awwrb3BlbnNoaWZ0LXNlcnZpY2Utc2VydmluZy1zaWduZXJAMTc2MTIwNDExMDAe
      Fw0yNTEwMjMwNzIxNDlaFw0yNzEyMjIwNzIxNTBaMDYxNDAyBgNVBAMMK29wZW5z
      aGlmdC1zZXJ2aWNlLXNlcnZpbmctc2lnbmVyQDE3NjEyMDQxMTAwggEiMA0GCSqG
      SIb3DQEBAQUAA4IBDwAwggEKAoIBAQCe1xxy04UT/Yzc0v3Sm8eXnnGK+07WdDao
      BPsfLLiycTxyM1OUHzOpBzRbu+2XFGJhSxY4a6S9TER99m6yE9x0UZSIAGIaq1YB
      pYN5QX0FYE3bhzUQdGi3GgVXzMpz6c5pFxCsF66R9Nns5SPoP83XxE1pSBoMM3Qt
      a2/nJLiIXGINLd+s8DrFi8ddamH12Oqm8KZc9pqIk5PD7SPSE07GQZ8pMFbe9zGe
      ncB1SvsA05G5Hc7duFQDjacL7A4CD5+Aq8bbqlSgpgDRmpaHcKMW4UhDuQwLR0Ip
      2CA6YB6BJq1P8DGr898hh7dDbVFqxOtBvv/Bvykt+Up1ARTuPF5rAgMBAAGjYzBh
      MA4GA1UdDwEB/wQEAwICpDAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBS38YUA
      C9VdFQtn49QZ6xwFbpIh1TAfBgNVHSMEGDAWgBS38YUAC9VdFQtn49QZ6xwFbpIh
      1TANBgkqhkiG9w0BAQsFAAOCAQEAj5RBFHvgmlxvF+Bth9SBRbeKCLEVRFQ0AiAy
      fBAH9ikHYlfan9Ma+VKGYfHnLtXkYJAVdNOWrc2Dvv/CQ9NweYldO8PwDaAI167X
      oAUsW6k98CUcheg9O1W0q3mjgousMzSWP6rC0jv49UUlQxjnI/q7UjZnfsFhuNZH
      cMOInDbPRVZ0yzaXaHx4dUKWMY52qoAttbXsQg+7i+lArdEnx/xDRbxyc9/4Pe2m
      bhIk261e4i3mvFN3JWta6CKazMCwBzQx/scAtvVskiLxVcZhVJ68jkdlTO9t0JEf
      OWhtB7g4lgJnWRMV11RwXW5F3ose58udJ/o4FzaXNwK/BeDfUA==
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    annotations:
      service.beta.openshift.io/inject-cabundle: "true"
    creationTimestamp: "2025-10-23T11:08:48Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data: {}
        f:metadata:
          f:annotations:
            .: {}
            f:service.beta.openshift.io/inject-cabundle: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-10-23T11:08:48Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          f:service-ca.crt: {}
      manager: service-ca-operator
      operation: Update
      time: "2025-10-23T11:08:48Z"
    name: openshift-service-ca.crt
    namespace: rhdh-operator
    resourceVersion: "39094"
    uid: 58780c72-1d3f-4029-86bf-32eb0a4d252d
- apiVersion: v1
  data:
    app-config.yaml: [REDACTED]
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: my-backstage-config-cm1 # placeholder for <bs>-default-appconfig
      data:
        default.app-config.yaml: |
          ###########################################################################################################
          # /!\ WARNING
          #
          # This is the default app-config file created and managed by the Operator for your CR.
          # Do NOT edit this manually in the Cluster, as your changes will be overridden by the Operator upon the
          # next reconciliation.
          # If you want to customize the application configuration, you should create your own app-config ConfigMap
          # and reference it in your CR.
          # See https://docs.redhat.com/en/documentation/red_hat_developer_hub/1.4/html/configuring/provisioning-and-using-your-custom-configuration#provisioning-your-custom-configuration
          # for more details.
          ###########################################################################################################
          backend:
            auth:
              externalAccess:
                - type: legacy
                  options:
                    subject: legacy-default-config
                    # This is a default value, which you should change by providing your own app-config
                    secret: "pl4s3Ch4ng3M3"
          auth:
            providers: {}
    db-secret.yaml: [REDACTED]
      apiVersion: v1
      kind: Secret
      metadata:
        name: postgres-secrets # will be replaced
      type: Opaque
      #stringData:
      #  POSTGRES_PASSWORD:
      #  POSTGRES_PORT: "5432"
      #  POSTGRES_USER: postgres
      #  POSTGRESQL_ADMIN_PASSWORD: admin123
      #  POSTGRES_HOST: bs1-db-service    #placeholder <crname>-db-service
    db-service.yaml: [REDACTED]
      apiVersion: v1
      kind: Service
      metadata:
        name: backstage-psql # placeholder for 'backstage-psql-<cr-name>' .NOTE: For the time it is static and linked to Secret-> postgres-secrets -> OSTGRES_HOST
      spec:
        selector:
          rhdh.redhat.com/app:  backstage-psql-cr1 # placeholder for 'backstage-psql-<cr-name>'
        clusterIP: None
        ports:
          - port: 5432
    db-statefulset.yaml: [REDACTED]
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: backstage-psql-cr1 # placeholder for 'backstage-psql-<cr-name>'
      spec:
        podManagementPolicy: OrderedReady
        # replicas: 1 # Intentionally omitted to allow HPA or custom scaling control.
        selector:
          matchLabels:
            rhdh.redhat.com/app: backstage-psql-cr1 # placeholder for 'backstage-psql-<cr-name>'
        serviceName: backstage-psql-cr1-hl # placeholder for 'backstage-psql-<cr-name>-hl'
        template:
          metadata:
            labels:
              rhdh.redhat.com/app: backstage-psql-cr1 # placeholder for 'backstage-psql-<cr-name>'
          spec:
            # fsGroup does not work for Openshift
            # AKS/EKS does not work w/o it
            #securityContext:
            #  fsGroup: 26
            automountServiceAccountToken: false
            ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
            ## The optional .spec.persistentVolumeClaimRetentionPolicy field controls if and how PVCs are deleted during the lifecycle of a StatefulSet.
            ## You must enable the StatefulSetAutoDeletePVC feature gate on the API server and the controller manager to use this field.
      #      persistentVolumeClaimRetentionPolicy:
      #        whenDeleted: Retain
      #        whenScaled: Retain
            containers:
              - env:
                  - name: POSTGRESQL_PORT_NUMBER
                    value: "5432"
                  - name: POSTGRESQL_VOLUME_DIR
                    value: /var/lib/pgsql/data
                  - name: PGDATA
                    value: /var/lib/pgsql/data/userdata
                image: registry.redhat.io/rhel9/postgresql-15@sha256:b8266c990ed58a483f7dc57406525d0910b694d65e6a7d00924231fe3d8f039e
                imagePullPolicy: IfNotPresent
                securityContext:
                  # runAsUser:26 does not work for Openshift but looks work for AKS/EKS
                  # runAsUser: 26
                  runAsGroup: 0
                  runAsNonRoot: true
                  allowPrivilegeEscalation: false
                  seccompProfile:
                    type: RuntimeDefault
                  capabilities:
                    drop:
                      - ALL
                livenessProbe:
                  exec:
                    command:
                      - /bin/sh
                      - -c
                      - exec pg_isready -U ${POSTGRES_USER} -h 127.0.0.1 -p 5432
                  failureThreshold: 6
                  initialDelaySeconds: 30
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 5
                name: postgresql
                ports:
                  - containerPort: 5432
                    name: tcp-postgresql
                    protocol: TCP
                readinessProbe:
                  exec:
                    command:
                      - /bin/sh
                      - -c
                      - -e
                      - |
                        exec pg_isready -U ${POSTGRES_USER} -h 127.0.0.1 -p 5432
                  failureThreshold: 6
                  initialDelaySeconds: 5
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 5
                resources:
                  requests:
                    cpu: 250m
                    memory: 256Mi
                  limits:
                    cpu: 250m
                    memory: 1024Mi
                    ephemeral-storage: 20Mi
                volumeMounts:
                  - mountPath: /dev/shm
                    name: dshm
                  - mountPath: /var/lib/pgsql/data
                    name: data
            restartPolicy: Always
            serviceAccountName: default
            volumes:
              - emptyDir:
                  medium: Memory
                name: dshm
        updateStrategy:
          rollingUpdate:
            partition: 0
          type: RollingUpdate
        volumeClaimTemplates:
          - apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: data
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi
    db-statefulset.yaml.k8s: [REDACTED]
      # if securityContext not present in AKS/EKS, the error is like this:
      # Error: EACCES: permission denied, open '/dynamic-plugins-root/backstage-plugin-scaffolder-backend-module-github-dynamic-0.2.2.tgz'
      # fsGroup doesn not work for Openshift
      spec:
        template:
          spec:
            securityContext:
              # any group id
              fsGroup: 1001
    deployment.yaml: [REDACTED]
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: backstage # placeholder for 'backstage-<cr-name>'
      spec:
        # replicas: 1 # Intentionally omitted to allow HPA or custom scaling control.
        selector:
          matchLabels:
            rhdh.redhat.com/app: # placeholder for 'backstage-<cr-name>'
        template:
          metadata:
            labels:
              rhdh.redhat.com/app: # placeholder for 'backstage-<cr-name>'
          spec:
            automountServiceAccountToken: false
            # if securityContext not present in AKS/EKS, the error is like this:
            #Error: EACCES: permission denied, open '/dynamic-plugins-root/backstage-plugin-scaffolder-backend-module-github-dynamic-0.2.2.tgz'
            # fsGroup doesn not work for Openshift
            #securityContext:
            #   fsGroup: 1001
            volumes:
              - ephemeral:
                  volumeClaimTemplate:
                    spec:
                      accessModes:
                        - ReadWriteOnce
                      resources:
                        requests:
                          storage: 2Gi
                name: dynamic-plugins-root
              # TODO Configuring dynamic plugins registry auth this way is deprecated and supported only for documentation backward compatibility.
              - name: dynamic-plugins-registry-auth
                secret:
                  defaultMode: 416
                  optional: true
                  secretName: dynamic-plugins-registry-auth
              - emptyDir: {}
                name: npmcacache
              - name: temp
                emptyDir: {}
            initContainers:
              - name: install-dynamic-plugins
                command:
                  - ./install-dynamic-plugins.sh
                  - /dynamic-plugins-root
                # image will be replaced by the value of the `RELATED_IMAGE_backstage` env var, if set
                image: registry.redhat.io/rhdh/rhdh-hub-rhel9@sha256:0c76994fde2cd71471802d458b682ab652be5a0e291c3a2773cb524259ac79b2
                imagePullPolicy: IfNotPresent
                securityContext:
                  readOnlyRootFilesystem: true
                  runAsNonRoot: true
                  allowPrivilegeEscalation: false
                  seccompProfile:
                    type: RuntimeDefault
                  capabilities:
                    drop:
                      - ALL
                env:
                  - name: NPM_CONFIG_USERCONFIG
                    value: /opt/app-root/src/.npmrc.dynamic-plugins/.npmrc
                  - name: MAX_ENTRY_SIZE
                    value: "30000000"
                volumeMounts:
                  - mountPath: /dynamic-plugins-root
                    name: dynamic-plugins-root
                  # TODO Configuring dynamic plugins registry auth this way is deprecated and supported only for documentation backward compatibility.
                  - mountPath: /opt/app-root/src/.config/containers
                    name: dynamic-plugins-registry-auth
                    readOnly: true
                  - mountPath: /opt/app-root/src/.npm/_cacache
                    name: npmcacache
                  - mountPath: /tmp
                    name: temp
                workingDir: /opt/app-root/src
                resources:
                  requests:
                    cpu: 250m
                    memory: 256Mi
                  limits:
                    cpu: 1000m
                    memory: 2.5Gi
                    ephemeral-storage: 5Gi
            containers:
              - name: backstage-backend
                # image will be replaced by the value of the `RELATED_IMAGE_backstage` env var, if set
                image: registry.redhat.io/rhdh/rhdh-hub-rhel9@sha256:0c76994fde2cd71471802d458b682ab652be5a0e291c3a2773cb524259ac79b2
                imagePullPolicy: IfNotPresent
                args:
                  - "--config"
                  - "dynamic-plugins-root/app-config.dynamic-plugins.yaml"
                securityContext:
                  capabilities:
                    drop:
                      - ALL
                  seccompProfile:
                    type: RuntimeDefault
                  runAsNonRoot: true
                  allowPrivilegeEscalation: false
                  readOnlyRootFilesystem: true
                startupProbe:
                  # This gives enough time upon container startup before the liveness and readiness probes are triggered.
                  # Giving (120s = initialDelaySeconds + failureThreshold * periodSeconds) to account for the worst case scenario.
                  httpGet:
                    path: /.backstage/health/v1/liveness
                    port: backend
                    scheme: HTTP
                  initialDelaySeconds: 30
                  timeoutSeconds: 4
                  periodSeconds: 20
                  successThreshold: 1
                  failureThreshold: 3
                readinessProbe:
                  failureThreshold: 3
                  httpGet:
                    path: /.backstage/health/v1/readiness
                    port: backend
                    scheme: HTTP
                  # Both liveness and readiness probes won't be triggered until the startup probe is successful.
                  # The startup probe is already configured to give enough time for the application to be started.
                  # So removing the additional delay here allows the readiness probe to be checked right away after the startup probe,
                  # which helps make the application available faster to the end-user.
                  #initialDelaySeconds: 30
                  periodSeconds: 10
                  successThreshold: 2
                  timeoutSeconds: 4
                livenessProbe:
                  failureThreshold: 3
                  httpGet:
                    path: /.backstage/health/v1/liveness
                    port: backend
                    scheme: HTTP
                  # Both liveness and readiness probes won't be triggered until the startup probe is successful.
                  # The startup probe is already configured to give enough time for the application to be started.
                  # So removing the additional delay here allows the readiness probe to be checked right away after the startup probe,
                  # which helps make the application available faster to the end-user.
                  #initialDelaySeconds: 60
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 4
                ports:
                  - name: backend
                    containerPort: 7007
                env:
                  - name: APP_CONFIG_backend_listen_port
                    value: "7007"
                volumeMounts:
                  - mountPath: /opt/app-root/src/dynamic-plugins-root
                    name: dynamic-plugins-root
                  - mountPath: /tmp
                    name: temp
                resources:
                  requests:
                    cpu: 250m
                    memory: 256Mi
                  limits:
                    cpu: 1000m
                    memory: 2.5Gi
                    ephemeral-storage: 5Gi
                workingDir: /opt/app-root/src
    deployment.yaml.k8s: [REDACTED]
      # if securityContext not present in AKS/EKS, the error is like this:
      # Error: EACCES: permission denied, open '/dynamic-plugins-root/backstage-plugin-scaffolder-backend-module-github-dynamic-0.2.2.tgz'
      # fsGroup doesn not work for Openshift
      spec:
        template:
          spec:
            securityContext:
              # any group id
              fsGroup: 1001
    dynamic-plugins.yaml: [REDACTED]
      default-dynamic-plugins #  must be the same as (deployment.yaml).spec.template.spec.volumes.name.dynamic-plugins-conf.configMap.name\n#data:\n#
      \ \"dynamic-plugins.yaml\": |\n#    ###########################################################################################################\n#
      \   # /!\\ WARNING\n#    #\n#    # This is the default dynamic plugins configuration
      file created and managed by the Operator for your CR.\n#    # Do NOT edit this
      manually in the Cluster, as your changes will be overridden by the Operator
      upon the\n#    # next reconciliation.\n#    # If you want to customize the dynamic
      plugins, you should create your own dynamic-plugins ConfigMap\n#    # and reference
      it in your CR.\n#    # See https://docs.redhat.com/en/documentation/red_hat_developer_hub/1.4/html/installing_and_viewing_plugins_in_red_hat_developer_hub/rhdh-installing-rhdh-plugins_title-plugins-rhdh-about#proc-config-dynamic-plugins-rhdh-operator_rhdh-installing-rhdh-plugins\n#
      \   # for more details or https://github.com/redhat-developer/rhdh-operator/blob/main/examples/rhdh-cr.yaml\n#
      \   # for an example.\n#    ###########################################################################################################\n#
      \   includes:\n#      - dynamic-plugins.default.yaml\n#    plugins: []\n#---\napiVersion:
      v1\nkind: ConfigMap\nmetadata:\n  name: default-dynamic-plugins\ndata:\n  dynamic-plugins.yaml:
      |\n    includes:\n      - dynamic-plugins.default.yaml\n    plugins:\n      -
      disabled: true\n        package: \"@redhat/backstage-plugin-orchestrator@1.6.0\"\n
      \       integrity: sha512-fOSJv2PgtD2urKwBM7p9W6gV/0UIHSf4pkZ9V/wQO0eg0Zi5Mys/CL1ba3nO9x9l84MX11UBZ2r7PPVJPrmOtw==\n
      \       pluginConfig:\n          dynamicPlugins:\n              frontend:\n
      \               red-hat-developer-hub.backstage-plugin-orchestrator:\n                  appIcons:\n
      \                   - importName: OrchestratorIcon\n                      name:
      orchestratorIcon\n                  dynamicRoutes:\n                    - importName:
      OrchestratorPage\n                      menuItem:\n                        icon:
      orchestratorIcon\n                        text: Orchestrator\n                      path:
      /orchestrator\n      - disabled: true\n        package: \"@redhat/backstage-plugin-orchestrator-backend-dynamic@1.6.0\"\n
      \       integrity: sha512-Kr55YbuVwEADwGef9o9wyimcgHmiwehPeAtVHa9g2RQYoSPEa6BeOlaPzB6W5Ke3M2bN/0j0XXtpLuvrlXQogA==\n
      \       pluginConfig:\n          orchestrator:\n            dataIndexService:\n
      \             url: http://sonataflow-platform-data-index-service\n        dependencies:\n
      \         - ref: sonataflow\n      - disabled: true\n        package: \"@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic@1.6.0\"\n
      \       integrity: sha512-Bueeix4661fXEnfJ9y31Yw91LXJgw6hJUG7lPVdESCi9VwBCjDB9Rm8u2yPqP8sriwr0OMtKtqD+Odn3LOPyVw==\n
      \       pluginConfig:\n          orchestrator:\n            dataIndexService:\n
      \             url: http://sonataflow-platform-data-index-service               \n
      \     - disabled: true\n        package: \"@redhat/backstage-plugin-orchestrator-form-widgets@1.6.0\"\n
      \       integrity: sha512-Tqn6HO21Q1TQ7TFUoRhwBVCtSBzbQYz+OaanzzIB0R24O6YtVx3wR7Chtr5TzC05Vz5GkBO1+FZid8BKpqljgA==\n
      \       pluginConfig:\n          dynamicPlugins:\n            frontend:\n              red-hat-developer-hub.backstage-plugin-orchestrator-form-widgets:
      { }"
    route.yaml: [REDACTED]
      apiVersion: route.openshift.io/v1
      kind: Route
      metadata:
        name: route # placeholder for 'backstage-<cr-name>'
      spec:
        port:
          targetPort: http-backend
        path: /
        tls:
          insecureEdgeTerminationPolicy: Redirect
          termination: edge
        to:
          kind: Service
          name:  # placeholder for 'backstage-<cr-name>'
    secret-files.yaml: [REDACTED]
      apiVersion: v1
      kind: Secret
      metadata:
        name: dynamic-plugins-npmrc
        annotations:
          rhdh.redhat.com/mount-path: /opt/app-root/src/.npmrc.dynamic-plugins
          rhdh.redhat.com/containers: install-dynamic-plugins
      type: Opaque
      stringData:
        .npmrc: |
          @redhat:registry=https://npm.registry.redhat.com
      #---
      # Placeholder for image registry ayth configuration for OCI dynamic plugins
      #apiVersion: v1
      #kind: Secret
      #metadata:
      #  name: dynamic-plugins-registry-auth
      #  annotations:
      #    rhdh.redhat.com/mount-path: /opt/app-root/src/.config/containers
      #    rhdh.redhat.com/containers: install-dynamic-plugins
      #type: Opaque


    service.yaml: [REDACTED]
      apiVersion: v1
      kind: Service
      metadata:
        name: backstage # placeholder for 'backstage-<cr-name>'
      spec:
        type: ClusterIP
        selector:
          rhdh.redhat.com/app:  # placeholder for 'backstage-<cr-name>'
        ports:
          - name: http-backend
            port: 80
            targetPort: backend
          - name: http-metrics
            protocol: TCP
            port: 9464
            targetPort: 9464
    service.yaml.k8s: [REDACTED]
      spec:
        type: NodePort
  kind: ConfigMap
  metadata:
    creationTimestamp: "2025-10-23T11:09:10Z"
    labels:
      olm.managed: "true"
      operators.coreos.com/rhdh.rhdh-operator: ""
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          .: {}
          f:app-config.yaml: {}
          f:db-secret.yaml: {}
          f:db-service.yaml: {}
          f:db-statefulset.yaml: {}
          f:db-statefulset.yaml.k8s: {}
          f:deployment.yaml: {}
          f:deployment.yaml.k8s: {}
          f:dynamic-plugins.yaml: {}
          f:route.yaml: {}
          f:secret-files.yaml: {}
          f:service.yaml: {}
          f:service.yaml.k8s: {}
        f:metadata:
          f:labels:
            .: {}
            f:olm.managed: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"09dc6fb2-852d-459e-8003-3456ca9c4bc6"}: {}
      manager: catalog
      operation: Update
      time: "2025-10-23T11:09:10Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            f:operators.coreos.com/rhdh.rhdh-operator: {}
      manager: olm
      operation: Update
      time: "2025-10-23T11:09:10Z"
    name: rhdh-default-config
    namespace: rhdh-operator
    ownerReferences:
    - apiVersion: operators.coreos.com/v1alpha1
      blockOwnerDeletion: false
      controller: false
      kind: ClusterServiceVersion
      name: rhdh-operator.v1.7.0
      uid: 09dc6fb2-852d-459e-8003-3456ca9c4bc6
    resourceVersion: "39246"
    uid: d6aac0c8-d63d-4588-935a-6a0d680da0da
- apiVersion: v1
  data:
    sonataflow.yaml: [REDACTED]
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: allow-knative-to-sonataflow-and-workflows # hardcoded
      spec:
        podSelector: {}
        ingress:
          - from:
              - namespaceSelector:
                  matchLabels:
                    # Allow knative events to be delivered to workflows.
                    kubernetes.io/metadata.name: knative-eventing
              - namespaceSelector:
                  matchLabels:
                    # Allow auxiliary knative function for workflow (such as m2k-save-transformation)
                    kubernetes.io/metadata.name: knative-serving
      ---
      # NetworkPolicy to unblock incoming traffic to the namespace
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: allow-external-communication # hardcoded
      spec:
        podSelector: {}
        ingress:
          - from:
              - namespaceSelector:
                  matchLabels:
                    # Allow knative events to be delivered to workflows.
                    policy-group.network.openshift.io/ingress: ""
      ---
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: allow-intra-network # hardcoded
      spec:
        # Apply this policy to all pods in the namespace
        podSelector: {}
        # Specify policy type as 'Ingress' to control incoming traffic rules
        policyTypes:
          - Ingress
        ingress:
          - from:
              # Allow ingress from any pod within the same namespace
              - podSelector: {}
      ---
      # NetworkPolicy to allow openshift-user-workload-monitoring pods to access all pods within the workflow's namespace
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: allow-monitoring-to-sonataflow-and-workflows # hardcoded
      spec:
        # Apply this policy to all pods in the namespace
        podSelector: {}
        # Specify policy type as 'Ingress' to control incoming traffic rules
        policyTypes:
          - Ingress
        ingress:
          - from:
              - namespaceSelector:
                  matchLabels:
                    # Allow openshift-user-workload-monitoring pods to access the workflow.
                    kubernetes.io/metadata.name: openshift-user-workload-monitoring
      ---
      apiVersion: sonataflow.org/v1alpha08
      kind: SonataFlowPlatform
      metadata:
        name: sonataflow-platform
      spec:
        monitoring:
          enabled: true
        services:
          dataIndex:
            enabled: true
            persistence:
              postgresql:
                secretRef:
                  name: backstage-psql-secret-{{backstage-name}}
                  userKey: POSTGRES_USER
                  passwordKey: POSTGRES_PASSWORD
                serviceRef:
                  name: backstage-psql-{{backstage-name}}
                  namespace: {{backstage-ns}}
                  databaseName: backstage_plugin_orchestrator
          jobService:
            enabled: true
            persistence:
              postgresql:
                secretRef:
                  name: backstage-psql-secret-{{backstage-name}}
                  userKey: POSTGRES_USER
                  passwordKey: POSTGRES_PASSWORD
                serviceRef:
                  name: backstage-psql-{{backstage-name}}
                  namespace: {{backstage-ns}}
                  databaseName: backstage_plugin_orchestrator
  kind: ConfigMap
  metadata:
    creationTimestamp: "2025-10-23T11:09:10Z"
    labels:
      olm.managed: "true"
      operators.coreos.com/rhdh.rhdh-operator: ""
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          .: {}
          f:sonataflow.yaml: {}
        f:metadata:
          f:labels:
            .: {}
            f:olm.managed: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"09dc6fb2-852d-459e-8003-3456ca9c4bc6"}: {}
      manager: catalog
      operation: Update
      time: "2025-10-23T11:09:10Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            f:operators.coreos.com/rhdh.rhdh-operator: {}
      manager: olm
      operation: Update
      time: "2025-10-23T11:09:10Z"
    name: rhdh-plugin-deps
    namespace: rhdh-operator
    ownerReferences:
    - apiVersion: operators.coreos.com/v1alpha1
      blockOwnerDeletion: false
      controller: false
      kind: ClusterServiceVersion
      name: rhdh-operator.v1.7.0
      uid: 09dc6fb2-852d-459e-8003-3456ca9c4bc6
    resourceVersion: "39251"
    uid: b810aa1b-3712-4bdd-b4f7-39860400f08e
kind: ConfigMapList
metadata:
  resourceVersion: "42057"
