---
apiVersion: v1
items:
- apiVersion: v1
  data:
    ca.crt: [REDACTED]
      -----BEGIN CERTIFICATE-----
      MIIBdzCCAR2gAwIBAgIBADAKBggqhkjOPQQDAjAjMSEwHwYDVQQDDBhrM3Mtc2Vy
      dmVyLWNhQDE3NjEyMTI0ODgwHhcNMjUxMDIzMDk0MTI4WhcNMzUxMDIxMDk0MTI4
      WjAjMSEwHwYDVQQDDBhrM3Mtc2VydmVyLWNhQDE3NjEyMTI0ODgwWTATBgcqhkjO
      PQIBBggqhkjOPQMBBwNCAAS/C3Lp2Ti3uT1D8toUxjfWqYnlL+XlM3XVwh6hKBsu
      rz/zOulFOQru61L0K9B50S66A7COZTq+0vUhjv9qhFq1o0IwQDAOBgNVHQ8BAf8E
      BAMCAqQwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUGX8P3XVae09GYfnLAxXL
      YQF4qxkwCgYIKoZIzj0EAwIDSAAwRQIhAP6iRjQNjPQHDri1Zhxb6ct+P/3C01pR
      Ty0QdNlJe6/4AiBt6u/nooOaKY9tZKn2zp62BQcbUOaO/fScIKkpiz1tEQ==
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    annotations:
      kubernetes.io/description: Contains a CA bundle that can be used to verify the
        kube-apiserver when using internal endpoints such as the internal service
        IP or kubernetes.default.svc. No other usage is guaranteed across distributions
        of Kubernetes clusters.
    creationTimestamp: "2025-10-23T09:43:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          .: {}
          f:ca.crt: {}
        f:metadata:
          f:annotations:
            .: {}
            f:kubernetes.io/description: {}
      manager: k3s
      operation: Update
      time: "2025-10-23T09:43:13Z"
    name: kube-root-ca.crt
    namespace: rhdh-operator
    resourceVersion: "1196"
    uid: 59026faa-36f5-4426-8418-d45077e2e7e5
- apiVersion: v1
  data:
    app-config.yaml: [REDACTED]
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: my-backstage-config-cm1 # placeholder for <bs>-default-appconfig
      data:
        default.app-config.yaml: |
          ###########################################################################################################
          # /!\ WARNING
          #
          # This is the default app-config file created and managed by the Operator for your CR.
          # Do NOT edit this manually in the Cluster, as your changes will be overridden by the Operator upon the
          # next reconciliation.
          # If you want to customize the application configuration, you should create your own app-config ConfigMap
          # and reference it in your CR.
          # See https://docs.redhat.com/en/documentation/red_hat_developer_hub/1.4/html/configuring/provisioning-and-using-your-custom-configuration#provisioning-your-custom-configuration
          # for more details.
          ###########################################################################################################
          backend:
            auth:
              externalAccess:
                - type: legacy
                  options:
                    subject: legacy-default-config
                    # This is a default value, which you should change by providing your own app-config
                    secret: "pl4s3Ch4ng3M3"
          auth:
            providers: {}
    db-secret.yaml: [REDACTED]
      apiVersion: v1
      kind: Secret
      metadata:
        name: postgres-secrets # will be replaced
      type: Opaque
      #stringData:
      #  POSTGRES_PASSWORD:
      #  POSTGRES_PORT: "5432"
      #  POSTGRES_USER: postgres
      #  POSTGRESQL_ADMIN_PASSWORD: admin123
      #  POSTGRES_HOST: bs1-db-service    #placeholder <crname>-db-service
    db-service.yaml: [REDACTED]
      apiVersion: v1
      kind: Service
      metadata:
        name: backstage-psql # placeholder for 'backstage-psql-<cr-name>' .NOTE: For the time it is static and linked to Secret-> postgres-secrets -> OSTGRES_HOST
      spec:
        selector:
          rhdh.redhat.com/app:  backstage-psql-cr1 # placeholder for 'backstage-psql-<cr-name>'
        clusterIP: None
        ports:
          - port: 5432
    db-statefulset.yaml: [REDACTED]
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: backstage-psql-cr1 # placeholder for 'backstage-psql-<cr-name>'
      spec:
        podManagementPolicy: OrderedReady
        # replicas: 1 # Intentionally omitted to allow HPA or custom scaling control.
        selector:
          matchLabels:
            rhdh.redhat.com/app: backstage-psql-cr1 # placeholder for 'backstage-psql-<cr-name>'
        serviceName: backstage-psql-cr1-hl # placeholder for 'backstage-psql-<cr-name>-hl'
        template:
          metadata:
            labels:
              rhdh.redhat.com/app: backstage-psql-cr1 # placeholder for 'backstage-psql-<cr-name>'
          spec:
            # fsGroup does not work for Openshift
            # AKS/EKS does not work w/o it
            #securityContext:
            #  fsGroup: 26
            automountServiceAccountToken: false
            ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
            ## The optional .spec.persistentVolumeClaimRetentionPolicy field controls if and how PVCs are deleted during the lifecycle of a StatefulSet.
            ## You must enable the StatefulSetAutoDeletePVC feature gate on the API server and the controller manager to use this field.
      #      persistentVolumeClaimRetentionPolicy:
      #        whenDeleted: Retain
      #        whenScaled: Retain
            containers:
              - env:
                  - name: POSTGRESQL_PORT_NUMBER
                    value: "5432"
                  - name: POSTGRESQL_VOLUME_DIR
                    value: /var/lib/pgsql/data
                  - name: PGDATA
                    value: /var/lib/pgsql/data/userdata
                image: quay.io/fedora/postgresql-15:latest # will be replaced with the actual image
                imagePullPolicy: IfNotPresent
                securityContext:
                  # runAsUser:26 does not work for Openshift but looks work for AKS/EKS
                  # runAsUser: 26
                  runAsGroup: 0
                  runAsNonRoot: true
                  allowPrivilegeEscalation: false
                  seccompProfile:
                    type: RuntimeDefault
                  capabilities:
                    drop:
                      - ALL
                livenessProbe:
                  exec:
                    command:
                      - /bin/sh
                      - -c
                      - exec pg_isready -U ${POSTGRES_USER} -h 127.0.0.1 -p 5432
                  failureThreshold: 6
                  initialDelaySeconds: 30
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 5
                name: postgresql
                ports:
                  - containerPort: 5432
                    name: tcp-postgresql
                    protocol: TCP
                readinessProbe:
                  exec:
                    command:
                      - /bin/sh
                      - -c
                      - -e
                      - |
                        exec pg_isready -U ${POSTGRES_USER} -h 127.0.0.1 -p 5432
                  failureThreshold: 6
                  initialDelaySeconds: 5
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 5
                resources:
                  requests:
                    cpu: 250m
                    memory: 256Mi
                  limits:
                    cpu: 250m
                    memory: 1024Mi
                    ephemeral-storage: 20Mi
                volumeMounts:
                  - mountPath: /dev/shm
                    name: dshm
                  - mountPath: /var/lib/pgsql/data
                    name: data
            restartPolicy: Always
            serviceAccountName: default
            volumes:
              - emptyDir:
                  medium: Memory
                name: dshm
        updateStrategy:
          rollingUpdate:
            partition: 0
          type: RollingUpdate
        volumeClaimTemplates:
          - apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: data
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi
    db-statefulset.yaml.k8s: [REDACTED]
      # if securityContext not present in AKS/EKS, the error is like this:
      # Error: EACCES: permission denied, open '/dynamic-plugins-root/backstage-plugin-scaffolder-backend-module-github-dynamic-0.2.2.tgz'
      # fsGroup doesn not work for Openshift
      spec:
        template:
          spec:
            securityContext:
              # any group id
              fsGroup: 1001
    deployment.yaml: [REDACTED]
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: backstage # placeholder for 'backstage-<cr-name>'
      spec:
        # replicas: 1 # Intentionally omitted to allow HPA or custom scaling control.
        selector:
          matchLabels:
            rhdh.redhat.com/app: # placeholder for 'backstage-<cr-name>'
        template:
          metadata:
            labels:
              rhdh.redhat.com/app: # placeholder for 'backstage-<cr-name>'
          spec:
            automountServiceAccountToken: false
            # if securityContext not present in AKS/EKS, the error is like this:
            #Error: EACCES: permission denied, open '/dynamic-plugins-root/backstage-plugin-scaffolder-backend-module-github-dynamic-0.2.2.tgz'
            # fsGroup doesn not work for Openshift
            #securityContext:
            #   fsGroup: 1001
            volumes:
              - ephemeral:
                  volumeClaimTemplate:
                    spec:
                      accessModes:
                        - ReadWriteOnce
                      resources:
                        requests:
                          storage: 2Gi
                name: dynamic-plugins-root
              # TODO Configuring dynamic plugins registry auth this way is deprecated and supported only for documentation backward compatibility.
              - name: dynamic-plugins-registry-auth
                secret:
                  defaultMode: 416
                  optional: true
                  secretName: dynamic-plugins-registry-auth
              - emptyDir: {}
                name: npmcacache
              - name: temp
                emptyDir: {}
            initContainers:
              - name: install-dynamic-plugins
                command:
                  - ./install-dynamic-plugins.sh
                  - /dynamic-plugins-root
                # image will be replaced by the value of the `RELATED_IMAGE_backstage` env var, if set
                image: quay.io/rhdh/rhdh-hub-rhel9:next
                imagePullPolicy: IfNotPresent
                securityContext:
                  readOnlyRootFilesystem: true
                  runAsNonRoot: true
                  allowPrivilegeEscalation: false
                  seccompProfile:
                    type: RuntimeDefault
                  capabilities:
                    drop:
                      - ALL
                env:
                  - name: NPM_CONFIG_USERCONFIG
                    value: /opt/app-root/src/.npmrc.dynamic-plugins/.npmrc
                  - name: MAX_ENTRY_SIZE
                    value: "30000000"
                volumeMounts:
                  - mountPath: /dynamic-plugins-root
                    name: dynamic-plugins-root
                  # TODO Configuring dynamic plugins registry auth this way is deprecated and supported only for documentation backward compatibility.
                  - mountPath: /opt/app-root/src/.config/containers
                    name: dynamic-plugins-registry-auth
                    readOnly: true
                  - mountPath: /opt/app-root/src/.npm/_cacache
                    name: npmcacache
                  - mountPath: /tmp
                    name: temp
                workingDir: /opt/app-root/src
                resources:
                  requests:
                    cpu: 250m
                    memory: 256Mi
                  limits:
                    cpu: 1000m
                    memory: 2.5Gi
                    ephemeral-storage: 5Gi
            containers:
              - name: backstage-backend
                # image will be replaced by the value of the `RELATED_IMAGE_backstage` env var, if set
                image: quay.io/rhdh/rhdh-hub-rhel9:next
                imagePullPolicy: IfNotPresent
                args:
                  - "--config"
                  - "dynamic-plugins-root/app-config.dynamic-plugins.yaml"
                securityContext:
                  capabilities:
                    drop:
                      - ALL
                  seccompProfile:
                    type: RuntimeDefault
                  runAsNonRoot: true
                  allowPrivilegeEscalation: false
                  readOnlyRootFilesystem: true
                startupProbe:
                  # This gives enough time upon container startup before the liveness and readiness probes are triggered.
                  # Giving (120s = initialDelaySeconds + failureThreshold * periodSeconds) to account for the worst case scenario.
                  httpGet:
                    path: /.backstage/health/v1/liveness
                    port: backend
                    scheme: HTTP
                  initialDelaySeconds: 30
                  timeoutSeconds: 4
                  periodSeconds: 20
                  successThreshold: 1
                  failureThreshold: 3
                readinessProbe:
                  failureThreshold: 3
                  httpGet:
                    path: /.backstage/health/v1/readiness
                    port: backend
                    scheme: HTTP
                  # Both liveness and readiness probes won't be triggered until the startup probe is successful.
                  # The startup probe is already configured to give enough time for the application to be started.
                  # So removing the additional delay here allows the readiness probe to be checked right away after the startup probe,
                  # which helps make the application available faster to the end-user.
                  #initialDelaySeconds: 30
                  periodSeconds: 10
                  successThreshold: 2
                  timeoutSeconds: 4
                livenessProbe:
                  failureThreshold: 3
                  httpGet:
                    path: /.backstage/health/v1/liveness
                    port: backend
                    scheme: HTTP
                  # Both liveness and readiness probes won't be triggered until the startup probe is successful.
                  # The startup probe is already configured to give enough time for the application to be started.
                  # So removing the additional delay here allows the readiness probe to be checked right away after the startup probe,
                  # which helps make the application available faster to the end-user.
                  #initialDelaySeconds: 60
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 4
                ports:
                  - name: backend
                    containerPort: 7007
                env:
                  - name: APP_CONFIG_backend_listen_port
                    value: "7007"
                volumeMounts:
                  - mountPath: /opt/app-root/src/dynamic-plugins-root
                    name: dynamic-plugins-root
                  - mountPath: /tmp
                    name: temp
                resources:
                  requests:
                    cpu: 250m
                    memory: 256Mi
                  limits:
                    cpu: 1000m
                    memory: 2.5Gi
                    ephemeral-storage: 5Gi
                workingDir: /opt/app-root/src
    deployment.yaml.k8s: [REDACTED]
      # if securityContext not present in AKS/EKS, the error is like this:
      # Error: EACCES: permission denied, open '/dynamic-plugins-root/backstage-plugin-scaffolder-backend-module-github-dynamic-0.2.2.tgz'
      # fsGroup doesn not work for Openshift
      spec:
        template:
          spec:
            securityContext:
              # any group id
              fsGroup: 1001
    dynamic-plugins.yaml: [REDACTED]
      default-dynamic-plugins #  must be the same as (deployment.yaml).spec.template.spec.volumes.name.dynamic-plugins-conf.configMap.name\n#data:\n#
      \ \"dynamic-plugins.yaml\": |\n#    ###########################################################################################################\n#
      \   # /!\\ WARNING\n#    #\n#    # This is the default dynamic plugins configuration
      file created and managed by the Operator for your CR.\n#    # Do NOT edit this
      manually in the Cluster, as your changes will be overridden by the Operator
      upon the\n#    # next reconciliation.\n#    # If you want to customize the dynamic
      plugins, you should create your own dynamic-plugins ConfigMap\n#    # and reference
      it in your CR.\n#    # See https://docs.redhat.com/en/documentation/red_hat_developer_hub/1.4/html/installing_and_viewing_plugins_in_red_hat_developer_hub/rhdh-installing-rhdh-plugins_title-plugins-rhdh-about#proc-config-dynamic-plugins-rhdh-operator_rhdh-installing-rhdh-plugins\n#
      \   # for more details or https://github.com/redhat-developer/rhdh-operator/blob/main/examples/rhdh-cr.yaml\n#
      \   # for an example.\n#    ###########################################################################################################\n#
      \   includes:\n#      - dynamic-plugins.default.yaml\n#    plugins: []\n#---\napiVersion:
      v1\nkind: ConfigMap\nmetadata:\n  name: default-dynamic-plugins\ndata:\n  dynamic-plugins.yaml:
      |\n    includes:\n      - dynamic-plugins.default.yaml\n    plugins:\n      -
      disabled: true\n        package: \"@redhat/backstage-plugin-orchestrator@1.7.1\"\n
      \       integrity: sha512-Cqu9EQwVQ4mpdgWTUA0MW89Gul0IklhvkkqVoO3CloQ1dnAj1XyXikCphzH5TmNDDd9K66dOpaKKCaW9KeJ4WA==\n
      \       pluginConfig:\n          dynamicPlugins:\n              frontend:\n
      \               red-hat-developer-hub.backstage-plugin-orchestrator:\n                  appIcons:\n
      \                   - importName: OrchestratorIcon\n                      name:
      orchestratorIcon\n                  dynamicRoutes:\n                    - importName:
      OrchestratorPage\n                      menuItem:\n                        icon:
      orchestratorIcon\n                        text: Orchestrator\n                      path:
      /orchestrator\n                  entityTabs:\n                    - path: /workflows\n
      \                     title: Workflows\n                      mountPoint: entity.page.workflows\n
      \                 mountPoints:\n                    - mountPoint: entity.page.workflows/cards\n
      \                     importName: OrchestratorCatalogTab\n                      config:\n
      \                       layout:\n                          gridColumn: '1 /
      -1'\n                          if:\n                            anyOf:\n                              -
      IsOrchestratorCatalogTabAvailable\n      - disabled: true\n        package:
      \"@redhat/backstage-plugin-orchestrator-backend-dynamic@1.7.1\"\n        integrity:
      sha512-9cXbedr0lC7ns7SNqARrWSQI4JGcZFw5xpfpUzA1tJaMMUjzAdPHTXqljf62/fs4hYBK8TJsWJ2KJkGVMzbrHQ==\n
      \       pluginConfig:\n          orchestrator:\n            dataIndexService:\n
      \             url: http://sonataflow-platform-data-index-service\n        dependencies:\n
      \         - ref: sonataflow\n      - disabled: true\n        package: \"@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic@1.7.1\"\n
      \       integrity: sha512-J1sTjA5kj6DphG8D65go9KlpIfKyLN/wq+XlY5Cb5djEo8mvF3wn3Haf60OGFo5cP4OfRSWqFwT7LM5/dNVwAg==\n
      \       pluginConfig:\n          orchestrator:\n            dataIndexService:\n
      \             url: http://sonataflow-platform-data-index-service               \n
      \     - disabled: true\n        package: \"@redhat/backstage-plugin-orchestrator-form-widgets@1.7.1\"\n
      \       integrity: sha512-0KIXrZoJ+O4xNNzN/zB4+VMuaRPuiUviAmM+fIhTo/P9aLA36F9aIlyMbUbki49uaJ0zd8KXMBvmJSHZNrYkGQ==\n
      \       pluginConfig:\n          dynamicPlugins:\n            frontend:\n              red-hat-developer-hub.backstage-plugin-orchestrator-form-widgets:
      { }"
    route.yaml: [REDACTED]
      apiVersion: route.openshift.io/v1
      kind: Route
      metadata:
        name: route # placeholder for 'backstage-<cr-name>'
      spec:
        port:
          targetPort: http-backend
        path: /
        tls:
          insecureEdgeTerminationPolicy: Redirect
          termination: edge
        to:
          kind: Service
          name:  # placeholder for 'backstage-<cr-name>'
    secret-files.yaml: [REDACTED]
      apiVersion: v1
      kind: Secret
      metadata:
        name: dynamic-plugins-npmrc
        annotations:
          rhdh.redhat.com/mount-path: /opt/app-root/src/.npmrc.dynamic-plugins
          rhdh.redhat.com/containers: install-dynamic-plugins
      type: Opaque
      stringData:
        .npmrc: |
          @redhat:registry=https://npm.registry.redhat.com
      #---
      # Placeholder for image registry ayth configuration for OCI dynamic plugins
      #apiVersion: v1
      #kind: Secret
      #metadata:
      #  name: dynamic-plugins-registry-auth
      #  annotations:
      #    rhdh.redhat.com/mount-path: /opt/app-root/src/.config/containers
      #    rhdh.redhat.com/containers: install-dynamic-plugins
      #type: Opaque


    service.yaml: [REDACTED]
      apiVersion: v1
      kind: Service
      metadata:
        name: backstage # placeholder for 'backstage-<cr-name>'
      spec:
        type: ClusterIP
        selector:
          rhdh.redhat.com/app:  # placeholder for 'backstage-<cr-name>'
        ports:
          - name: http-backend
            port: 80
            targetPort: backend
          - name: http-metrics
            protocol: TCP
            port: 9464
            targetPort: 9464
    service.yaml.k8s: [REDACTED]
      spec:
        type: NodePort
  kind: ConfigMap
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","data":{"app-config.yaml":"apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-backstage-config-cm1 # placeholder for \u003cbs\u003e-default-appconfig\ndata:\n  default.app-config.yaml: |\n    ###########################################################################################################\n    # /!\\ WARNING\n    #\n    # This is the default app-config file created and managed by the Operator for your CR.\n    # Do NOT edit this manually in the Cluster, as your changes will be overridden by the Operator upon the\n    # next reconciliation.\n    # If you want to customize the application configuration, you should create your own app-config ConfigMap\n    # and reference it in your CR.\n    # See https://docs.redhat.com/en/documentation/red_hat_developer_hub/1.4/html/configuring/provisioning-and-using-your-custom-configuration#provisioning-your-custom-configuration\n    # for more details.\n    ###########################################################################################################\n    backend:\n      auth:\n        externalAccess:\n          - type: legacy\n            options:\n              subject: legacy-default-config\n              # This is a default value, which you should change by providing your own app-config\n              secret: \"pl4s3Ch4ng3M3\"\n    auth:\n      providers: {}\n","db-secret.yaml":"apiVersion: v1\nkind: Secret\nmetadata:\n  name: postgres-secrets # will be replaced\ntype: Opaque\n#stringData:\n#  POSTGRES_PASSWORD:\n#  POSTGRES_PORT: \"5432\"\n#  POSTGRES_USER: postgres\n#  POSTGRESQL_ADMIN_PASSWORD: admin123\n#  POSTGRES_HOST: bs1-db-service    #placeholder \u003ccrname\u003e-db-service","db-service.yaml":"apiVersion: v1\nkind: Service\nmetadata:\n  name: backstage-psql # placeholder for 'backstage-psql-\u003ccr-name\u003e' .NOTE: For the time it is static and linked to Secret-\u003e postgres-secrets -\u003e OSTGRES_HOST\nspec:\n  selector:\n    rhdh.redhat.com/app:  backstage-psql-cr1 # placeholder for 'backstage-psql-\u003ccr-name\u003e'\n  clusterIP: None\n  ports:\n    - port: 5432\n","db-statefulset.yaml":"apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: backstage-psql-cr1 # placeholder for 'backstage-psql-\u003ccr-name\u003e'\nspec:\n  podManagementPolicy: OrderedReady\n  # replicas: 1 # Intentionally omitted to allow HPA or custom scaling control.\n  selector:\n    matchLabels:\n      rhdh.redhat.com/app: backstage-psql-cr1 # placeholder for 'backstage-psql-\u003ccr-name\u003e'\n  serviceName: backstage-psql-cr1-hl # placeholder for 'backstage-psql-\u003ccr-name\u003e-hl'\n  template:\n    metadata:\n      labels:\n        rhdh.redhat.com/app: backstage-psql-cr1 # placeholder for 'backstage-psql-\u003ccr-name\u003e'\n    spec:\n      # fsGroup does not work for Openshift\n      # AKS/EKS does not work w/o it\n      #securityContext:\n      #  fsGroup: 26\n      automountServiceAccountToken: false\n      ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\n      ## The optional .spec.persistentVolumeClaimRetentionPolicy field controls if and how PVCs are deleted during the lifecycle of a StatefulSet.\n      ## You must enable the StatefulSetAutoDeletePVC feature gate on the API server and the controller manager to use this field.\n#      persistentVolumeClaimRetentionPolicy:\n#        whenDeleted: Retain\n#        whenScaled: Retain\n      containers:\n        - env:\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: /var/lib/pgsql/data\n            - name: PGDATA\n              value: /var/lib/pgsql/data/userdata\n          image: quay.io/fedora/postgresql-15:latest # will be replaced with the actual image\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            # runAsUser:26 does not work for Openshift but looks work for AKS/EKS\n            # runAsUser: 26\n            runAsGroup: 0\n            runAsNonRoot: true\n            allowPrivilegeEscalation: false\n            seccompProfile:\n              type: RuntimeDefault\n            capabilities:\n              drop:\n                - ALL\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U ${POSTGRES_USER} -h 127.0.0.1 -p 5432\n            failureThreshold: 6\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 5\n          name: postgresql\n          ports:\n            - containerPort: 5432\n              name: tcp-postgresql\n              protocol: TCP\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U ${POSTGRES_USER} -h 127.0.0.1 -p 5432\n            failureThreshold: 6\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 5\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n            limits:\n              cpu: 250m\n              memory: 1024Mi\n              ephemeral-storage: 20Mi\n          volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n            - mountPath: /var/lib/pgsql/data\n              name: data\n      restartPolicy: Always\n      serviceAccountName: default\n      volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n  updateStrategy:\n    rollingUpdate:\n      partition: 0\n    type: RollingUpdate\n  volumeClaimTemplates:\n    - apiVersion: v1\n      kind: PersistentVolumeClaim\n      metadata:\n        name: data\n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 1Gi","db-statefulset.yaml.k8s":"# if securityContext not present in AKS/EKS, the error is like this:\n# Error: EACCES: permission denied, open '/dynamic-plugins-root/backstage-plugin-scaffolder-backend-module-github-dynamic-0.2.2.tgz'\n# fsGroup doesn not work for Openshift\nspec:\n  template:\n    spec:\n      securityContext:\n        # any group id\n        fsGroup: 1001","deployment.yaml":"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backstage # placeholder for 'backstage-\u003ccr-name\u003e'\nspec:\n  # replicas: 1 # Intentionally omitted to allow HPA or custom scaling control.\n  selector:\n    matchLabels:\n      rhdh.redhat.com/app: # placeholder for 'backstage-\u003ccr-name\u003e'\n  template:\n    metadata:\n      labels:\n        rhdh.redhat.com/app: # placeholder for 'backstage-\u003ccr-name\u003e'\n    spec:\n      automountServiceAccountToken: false\n      # if securityContext not present in AKS/EKS, the error is like this:\n      #Error: EACCES: permission denied, open '/dynamic-plugins-root/backstage-plugin-scaffolder-backend-module-github-dynamic-0.2.2.tgz'\n      # fsGroup doesn not work for Openshift\n      #securityContext:\n      #   fsGroup: 1001\n      volumes:\n        - ephemeral:\n            volumeClaimTemplate:\n              spec:\n                accessModes:\n                  - ReadWriteOnce\n                resources:\n                  requests:\n                    storage: 2Gi\n          name: dynamic-plugins-root\n        # TODO Configuring dynamic plugins registry auth this way is deprecated and supported only for documentation backward compatibility.\n        - name: dynamic-plugins-registry-auth\n          secret:\n            defaultMode: 416\n            optional: true\n            secretName: dynamic-plugins-registry-auth\n        - emptyDir: {}\n          name: npmcacache\n        - name: temp\n          emptyDir: {}\n      initContainers:\n        - name: install-dynamic-plugins\n          command:\n            - ./install-dynamic-plugins.sh\n            - /dynamic-plugins-root\n          # image will be replaced by the value of the `RELATED_IMAGE_backstage` env var, if set\n          image: quay.io/rhdh/rhdh-hub-rhel9:next\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            readOnlyRootFilesystem: true\n            runAsNonRoot: true\n            allowPrivilegeEscalation: false\n            seccompProfile:\n              type: RuntimeDefault\n            capabilities:\n              drop:\n                - ALL\n          env:\n            - name: NPM_CONFIG_USERCONFIG\n              value: /opt/app-root/src/.npmrc.dynamic-plugins/.npmrc\n            - name: MAX_ENTRY_SIZE\n              value: \"30000000\"\n          volumeMounts:\n            - mountPath: /dynamic-plugins-root\n              name: dynamic-plugins-root\n            # TODO Configuring dynamic plugins registry auth this way is deprecated and supported only for documentation backward compatibility.\n            - mountPath: /opt/app-root/src/.config/containers\n              name: dynamic-plugins-registry-auth\n              readOnly: true\n            - mountPath: /opt/app-root/src/.npm/_cacache\n              name: npmcacache\n            - mountPath: /tmp\n              name: temp\n          workingDir: /opt/app-root/src\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n            limits:\n              cpu: 1000m\n              memory: 2.5Gi\n              ephemeral-storage: 5Gi\n      containers:\n        - name: backstage-backend\n          # image will be replaced by the value of the `RELATED_IMAGE_backstage` env var, if set\n          image: quay.io/rhdh/rhdh-hub-rhel9:next\n          imagePullPolicy: IfNotPresent\n          args:\n            - \"--config\"\n            - \"dynamic-plugins-root/app-config.dynamic-plugins.yaml\"\n          securityContext:\n            capabilities:\n              drop:\n                - ALL\n            seccompProfile:\n              type: RuntimeDefault\n            runAsNonRoot: true\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          startupProbe:\n            # This gives enough time upon container startup before the liveness and readiness probes are triggered.\n            # Giving (120s = initialDelaySeconds + failureThreshold * periodSeconds) to account for the worst case scenario.\n            httpGet:\n              path: /.backstage/health/v1/liveness\n              port: backend\n              scheme: HTTP\n            initialDelaySeconds: 30\n            timeoutSeconds: 4\n            periodSeconds: 20\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /.backstage/health/v1/readiness\n              port: backend\n              scheme: HTTP\n            # Both liveness and readiness probes won't be triggered until the startup probe is successful.\n            # The startup probe is already configured to give enough time for the application to be started.\n            # So removing the additional delay here allows the readiness probe to be checked right away after the startup probe,\n            # which helps make the application available faster to the end-user.\n            #initialDelaySeconds: 30\n            periodSeconds: 10\n            successThreshold: 2\n            timeoutSeconds: 4\n          livenessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /.backstage/health/v1/liveness\n              port: backend\n              scheme: HTTP\n            # Both liveness and readiness probes won't be triggered until the startup probe is successful.\n            # The startup probe is already configured to give enough time for the application to be started.\n            # So removing the additional delay here allows the readiness probe to be checked right away after the startup probe,\n            # which helps make the application available faster to the end-user.\n            #initialDelaySeconds: 60\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 4\n          ports:\n            - name: backend\n              containerPort: 7007\n          env:\n            - name: APP_CONFIG_backend_listen_port\n              value: \"7007\"\n          volumeMounts:\n            - mountPath: /opt/app-root/src/dynamic-plugins-root\n              name: dynamic-plugins-root\n            - mountPath: /tmp\n              name: temp\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n            limits:\n              cpu: 1000m\n              memory: 2.5Gi\n              ephemeral-storage: 5Gi\n          workingDir: /opt/app-root/src\n","deployment.yaml.k8s":"# if securityContext not present in AKS/EKS, the error is like this:\n# Error: EACCES: permission denied, open '/dynamic-plugins-root/backstage-plugin-scaffolder-backend-module-github-dynamic-0.2.2.tgz'\n# fsGroup doesn not work for Openshift\nspec:\n  template:\n    spec:\n      securityContext:\n        # any group id\n        fsGroup: 1001","dynamic-plugins.yaml":"#apiVersion: v1\n#kind: ConfigMap\n#metadata:\n#  name: default-dynamic-plugins #  must be the same as (deployment.yaml).spec.template.spec.volumes.name.dynamic-plugins-conf.configMap.name\n#data:\n#  \"dynamic-plugins.yaml\": |\n#    ###########################################################################################################\n#    # /!\\ WARNING\n#    #\n#    # This is the default dynamic plugins configuration file created and managed by the Operator for your CR.\n#    # Do NOT edit this manually in the Cluster, as your changes will be overridden by the Operator upon the\n#    # next reconciliation.\n#    # If you want to customize the dynamic plugins, you should create your own dynamic-plugins ConfigMap\n#    # and reference it in your CR.\n#    # See https://docs.redhat.com/en/documentation/red_hat_developer_hub/1.4/html/installing_and_viewing_plugins_in_red_hat_developer_hub/rhdh-installing-rhdh-plugins_title-plugins-rhdh-about#proc-config-dynamic-plugins-rhdh-operator_rhdh-installing-rhdh-plugins\n#    # for more details or https://github.com/redhat-developer/rhdh-operator/blob/main/examples/rhdh-cr.yaml\n#    # for an example.\n#    ###########################################################################################################\n#    includes:\n#      - dynamic-plugins.default.yaml\n#    plugins: []\n#---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: default-dynamic-plugins\ndata:\n  dynamic-plugins.yaml: |\n    includes:\n      - dynamic-plugins.default.yaml\n    plugins:\n      - disabled: true\n        package: \"@redhat/backstage-plugin-orchestrator@1.7.1\"\n        integrity: sha512-Cqu9EQwVQ4mpdgWTUA0MW89Gul0IklhvkkqVoO3CloQ1dnAj1XyXikCphzH5TmNDDd9K66dOpaKKCaW9KeJ4WA==\n        pluginConfig:\n          dynamicPlugins:\n              frontend:\n                red-hat-developer-hub.backstage-plugin-orchestrator:\n                  appIcons:\n                    - importName: OrchestratorIcon\n                      name: orchestratorIcon\n                  dynamicRoutes:\n                    - importName: OrchestratorPage\n                      menuItem:\n                        icon: orchestratorIcon\n                        text: Orchestrator\n                      path: /orchestrator\n                  entityTabs:\n                    - path: /workflows\n                      title: Workflows\n                      mountPoint: entity.page.workflows\n                  mountPoints:\n                    - mountPoint: entity.page.workflows/cards\n                      importName: OrchestratorCatalogTab\n                      config:\n                        layout:\n                          gridColumn: '1 / -1'\n                          if:\n                            anyOf:\n                              - IsOrchestratorCatalogTabAvailable\n      - disabled: true\n        package: \"@redhat/backstage-plugin-orchestrator-backend-dynamic@1.7.1\"\n        integrity: sha512-9cXbedr0lC7ns7SNqARrWSQI4JGcZFw5xpfpUzA1tJaMMUjzAdPHTXqljf62/fs4hYBK8TJsWJ2KJkGVMzbrHQ==\n        pluginConfig:\n          orchestrator:\n            dataIndexService:\n              url: http://sonataflow-platform-data-index-service\n        dependencies:\n          - ref: sonataflow\n      - disabled: true\n        package: \"@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic@1.7.1\"\n        integrity: sha512-J1sTjA5kj6DphG8D65go9KlpIfKyLN/wq+XlY5Cb5djEo8mvF3wn3Haf60OGFo5cP4OfRSWqFwT7LM5/dNVwAg==\n        pluginConfig:\n          orchestrator:\n            dataIndexService:\n              url: http://sonataflow-platform-data-index-service               \n      - disabled: true\n        package: \"@redhat/backstage-plugin-orchestrator-form-widgets@1.7.1\"\n        integrity: sha512-0KIXrZoJ+O4xNNzN/zB4+VMuaRPuiUviAmM+fIhTo/P9aLA36F9aIlyMbUbki49uaJ0zd8KXMBvmJSHZNrYkGQ==\n        pluginConfig:\n          dynamicPlugins:\n            frontend:\n              red-hat-developer-hub.backstage-plugin-orchestrator-form-widgets: { }","route.yaml":"apiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: route # placeholder for 'backstage-\u003ccr-name\u003e'\nspec:\n  port:\n    targetPort: http-backend\n  path: /\n  tls:\n    insecureEdgeTerminationPolicy: Redirect\n    termination: edge\n  to:\n    kind: Service\n    name:  # placeholder for 'backstage-\u003ccr-name\u003e'","secret-files.yaml":"apiVersion: v1\nkind: Secret\nmetadata:\n  name: dynamic-plugins-npmrc\n  annotations:\n    rhdh.redhat.com/mount-path: /opt/app-root/src/.npmrc.dynamic-plugins\n    rhdh.redhat.com/containers: install-dynamic-plugins\ntype: Opaque\nstringData:\n  .npmrc: |\n    @redhat:registry=https://npm.registry.redhat.com\n#---\n# Placeholder for image registry ayth configuration for OCI dynamic plugins\n#apiVersion: v1\n#kind: Secret\n#metadata:\n#  name: dynamic-plugins-registry-auth\n#  annotations:\n#    rhdh.redhat.com/mount-path: /opt/app-root/src/.config/containers\n#    rhdh.redhat.com/containers: install-dynamic-plugins\n#type: Opaque\n\n\n","service.yaml":"apiVersion: v1\nkind: Service\nmetadata:\n  name: backstage # placeholder for 'backstage-\u003ccr-name\u003e'\nspec:\n  type: ClusterIP\n  selector:\n    rhdh.redhat.com/app:  # placeholder for 'backstage-\u003ccr-name\u003e'\n  ports:\n    - name: http-backend\n      port: 80\n      targetPort: backend\n    - name: http-metrics\n      protocol: TCP\n      port: 9464\n      targetPort: 9464","service.yaml.k8s":"spec:\n  type: NodePort\n"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"rhdh-default-config","namespace":"rhdh-operator"}}
    creationTimestamp: "2025-10-23T09:43:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          .: {}
          f:app-config.yaml: {}
          f:db-secret.yaml: {}
          f:db-service.yaml: {}
          f:db-statefulset.yaml: {}
          f:db-statefulset.yaml.k8s: {}
          f:deployment.yaml: {}
          f:deployment.yaml.k8s: {}
          f:dynamic-plugins.yaml: {}
          f:route.yaml: {}
          f:secret-files.yaml: {}
          f:service.yaml: {}
          f:service.yaml.k8s: {}
        f:metadata:
          f:annotations:
            .: {}
            f:kubectl.kubernetes.io/last-applied-configuration: {}
      manager: kubectl-client-side-apply
      operation: Update
      time: "2025-10-23T09:43:13Z"
    name: rhdh-default-config
    namespace: rhdh-operator
    resourceVersion: "1216"
    uid: df977f44-e173-429b-8a47-d66c187e8263
- apiVersion: v1
  data:
    argocd.yaml: [REDACTED]
      ---
      apiVersion: argoproj.io/v1alpha1
      kind: AppProject
      metadata:
        name: argocd-app-project
      spec:
        destinations:
        - name: '*'
          namespace: '*'
          server: '*'
        sourceRepos:
        - '*'
    sonataflow.yaml: [REDACTED]
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: allow-infra-ns-to-workflow-ns # hardcoded
      spec:
        podSelector: {}
        ingress:
          - from:
              - namespaceSelector:
                  matchLabels:
                    # Allow knative events to be delivered to workflows.
                    kubernetes.io/metadata.name: knative-eventing
              - namespaceSelector:
                  matchLabels:
                    # Allow auxiliary knative function for workflow (such as m2k-save-transformation)
                    kubernetes.io/metadata.name: knative-serving
              - namespaceSelector:
                  matchLabels:
                    # Allow openshift serverless logic operator controller pod to access all pods in sonataflow
                    kubernetes.io/metadata.name: openshift-serverless-logic
      ---
      # NetworkPolicy to unblock incoming traffic to the namespace
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: allow-external-communication # hardcoded
      spec:
        podSelector: {}
        ingress:
          - from:
              - namespaceSelector:
                  matchLabels:
                    # Allow knative events to be delivered to workflows.
                    policy-group.network.openshift.io/ingress: ""
      ---
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: allow-intra-network # hardcoded
      spec:
        # Apply this policy to all pods in the namespace
        podSelector: {}
        # Specify policy type as 'Ingress' to control incoming traffic rules
        policyTypes:
          - Ingress
        ingress:
          - from:
              # Allow ingress from any pod within the same namespace
              - podSelector: {}
      ---
      # NetworkPolicy to allow openshift-user-workload-monitoring pods to access all pods within the workflow's namespace
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: allow-monitoring-to-sonataflow-and-workflows # hardcoded
      spec:
        # Apply this policy to all pods in the namespace
        podSelector: {}
        # Specify policy type as 'Ingress' to control incoming traffic rules
        policyTypes:
          - Ingress
        ingress:
          - from:
              - namespaceSelector:
                  matchLabels:
                    # Allow openshift-user-workload-monitoring pods to access the workflow.
                    kubernetes.io/metadata.name: openshift-user-workload-monitoring
      ---
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: create-sonataflow-database-{{backstage-name}}
      spec:
        ttlSecondsAfterFinished: 30
        activeDeadlineSeconds: 120
        template:
          spec:
            containers:
              - name: psql
                image: quay.io/fedora/postgresql-15:latest
                resources:
                  limits:
                    cpu: "100m"
                    memory: "128Mi"
                  requests:
                    cpu: "100m"
                    memory: "64Mi"
                securityContext:
                  readOnlyRootFilesystem: true
                  allowPrivilegeEscalation: false
                  runAsNonRoot: true
                  capabilities:
                    drop:
                      - ALL
                envFrom:
                  - secretRef:
                      name: backstage-psql-secret-{{backstage-name}}
                command: [ "sh", "-c" ]
                args:
                  - |
                    set -e
                    DB_EXISTS=$(PGPASSWORD=${POSTGRES_PASSWORD} psql -h ${POSTGRES_HOST} -p ${POSTGRES_PORT} -U ${POSTGRES_USER} -tAc "SELECT 1 FROM pg_database WHERE datname='backstage_plugin_orchestrator'" postgres)
                    if [ -z "$DB_EXISTS" ]; then
                      PGPASSWORD=${POSTGRES_PASSWORD} psql -h ${POSTGRES_HOST} -p ${POSTGRES_PORT} -U ${POSTGRES_USER} -c "CREATE DATABASE backstage_plugin_orchestrator;" postgres
                    fi
            restartPolicy: Never
      ---
      apiVersion: sonataflow.org/v1alpha08
      kind: SonataFlowPlatform
      metadata:
        name: sonataflow-platform
      spec:
        monitoring:
          enabled: true
        services:
          dataIndex:
            enabled: true
            persistence:
              postgresql:
                secretRef:
                  name: backstage-psql-secret-{{backstage-name}}
                  userKey: POSTGRES_USER
                  passwordKey: POSTGRES_PASSWORD
                serviceRef:
                  name: backstage-psql-{{backstage-name}}
                  namespace: {{backstage-ns}}
                  databaseName: backstage_plugin_orchestrator
          jobService:
            enabled: true
            persistence:
              postgresql:
                secretRef:
                  name: backstage-psql-secret-{{backstage-name}}
                  userKey: POSTGRES_USER
                  passwordKey: POSTGRES_PASSWORD
                serviceRef:
                  name: backstage-psql-{{backstage-name}}
                  namespace: {{backstage-ns}}
                  databaseName: backstage_plugin_orchestrator
    tekton.yaml: [REDACTED]
      ---
      apiVersion: tekton.dev/v1
      kind: Task
      metadata:
        name: git-cli
        namespace: {{backstage-ns}}
        labels:
          app.kubernetes.io/version: "0.4"
        annotations:
          tekton.dev/pipelines.minVersion: "0.21.0"
          tekton.dev/categories: Git
          tekton.dev/tags: git
          tekton.dev/displayName: "git cli"
          tekton.dev/platforms: "linux/amd64,linux/s390x,linux/ppc64le"
      spec:
        description: >-
          This task can be used to perform git operations.

          Git command that needs to be run can be passed as a script to
          the task. This task needs authentication to git in order to push
          after the git operation.

        workspaces:
          - name: source
            description: A workspace that contains the fetched git repository.

          - name: input
            optional: true
            description: |
              An optional workspace that contains the files that need to be added to git. You can
              access the workspace from your script using `$(workspaces.input.path)`, for instance:

              cp $(workspaces.input.path)/file_that_i_want .
              git add file_that_i_want
              # etc

          - name: ssh-directory
            optional: true
            description: |
              A .ssh directory with private key, known_hosts, config, etc. Copied to
              the user's home before git commands are executed. Used to authenticate
              with the git remote when performing the clone. Binding a Secret to this
              Workspace is strongly recommended over other volume types.

          - name: basic-auth
            optional: true
            description: |
              A Workspace containing a .gitconfig and .git-credentials file. These
              will be copied to the user's home before any git commands are run. Any
              other files in this Workspace are ignored. It is strongly recommended
              to use ssh-directory over basic-auth whenever possible and to bind a
              Secret to this Workspace over other volume types.
        params:
          - name: BASE_IMAGE
            description: |
              The base image for the task.
            type: string
            # TODO: Deprecate use of root image.
            default: cgr.dev/chainguard/git:root-2.39@sha256:7759f87050dd8bacabe61354d75ccd7f864d6b6f8ec42697db7159eccd491139

          - name: GIT_USER_NAME
            type: string
            description: |
              Git user name for performing git operation.
            default: ""

          - name: GIT_USER_EMAIL
            type: string
            description: |
              Git user email for performing git operation.
            default: ""

          - name: GIT_SCRIPT
            description: The git script to run.
            type: string
            default: |
              git help

          - name: USER_HOME
            description: |
              Absolute path to the user's home directory. Set this explicitly if you are running the image as a non-root user or have overridden
              the gitInitImage param with an image containing custom user configuration.
            type: string
            default: "/root"

          - name: VERBOSE
            description: Log the commands that are executed during `git-clone`'s operation.
            type: string
            default: "true"

        results:
          - name: commit
            description: The precise commit SHA after the git operation.

        steps:
          - name: git
            image: $(params.BASE_IMAGE)
            workingDir: $(workspaces.source.path)
            env:
            - name: HOME
              value: $(params.USER_HOME)
            - name: PARAM_VERBOSE
              value: $(params.VERBOSE)
            - name: PARAM_USER_HOME
              value: $(params.USER_HOME)
            - name: WORKSPACE_OUTPUT_PATH
              value: $(workspaces.output.path)
            - name: WORKSPACE_SSH_DIRECTORY_BOUND
              value: $(workspaces.ssh-directory.bound)
            - name: WORKSPACE_SSH_DIRECTORY_PATH
              value: $(workspaces.ssh-directory.path)
            - name: WORKSPACE_BASIC_AUTH_DIRECTORY_BOUND
              value: $(workspaces.basic-auth.bound)
            - name: WORKSPACE_BASIC_AUTH_DIRECTORY_PATH
              value: $(workspaces.basic-auth.path)
            script: |
              #!/usr/bin/env sh
              set -eu

              if [ "${PARAM_VERBOSE}" = "true" ] ; then
                set -x
              fi

              if [ "${WORKSPACE_BASIC_AUTH_DIRECTORY_BOUND}" = "true" ] ; then
                cp "${WORKSPACE_BASIC_AUTH_DIRECTORY_PATH}/.git-credentials" "${PARAM_USER_HOME}/.git-credentials"
                cp "${WORKSPACE_BASIC_AUTH_DIRECTORY_PATH}/.gitconfig" "${PARAM_USER_HOME}/.gitconfig"
                chmod 400 "${PARAM_USER_HOME}/.git-credentials"
                chmod 400 "${PARAM_USER_HOME}/.gitconfig"
              fi

              if [ "${WORKSPACE_SSH_DIRECTORY_BOUND}" = "true" ] ; then
                cp -R "${WORKSPACE_SSH_DIRECTORY_PATH}" "${PARAM_USER_HOME}"/.ssh
                chmod 700 "${PARAM_USER_HOME}"/.ssh
                chmod -R 400 "${PARAM_USER_HOME}"/.ssh/*
              fi

              # Setting up the config for the git.
              git config --global user.email "$(params.GIT_USER_EMAIL)"
              git config --global user.name "$(params.GIT_USER_NAME)"

              eval '$(params.GIT_SCRIPT)'

              RESULT_SHA="$(git rev-parse HEAD | tr -d '\n')"
              EXIT_CODE="$?"
              if [ "$EXIT_CODE" != 0 ]
              then
                exit $EXIT_CODE
              fi
              # Make sure we don't add a trailing newline to the result!
              printf "%s" "$RESULT_SHA" > "$(results.commit.path)"
            # Patch to apply on OpenShift
            securityContext:
              runAsNonRoot: true
              runAsUser: 65532
      ---
      apiVersion: tekton.dev/v1
      kind: Task
      metadata:
        name: flattener
        namespace: {{backstage-ns}}
      spec:
        workspaces:
          - name: workflow-source
        params:
          - name: workflowId
            description: The workflow ID from the repository
            type: string
          - name: convertToFlat
            description: Whether conversion to flat layout is needed or it's already flattened
            type: string
            default: "true"
        steps:
          - name: flatten
            image: registry.access.redhat.com/ubi9-minimal
            workingDir: $(workspaces.workflow-source.path)
            script: |
              ROOT=/workspace/workflow
              TARGET=flat
              mkdir -p flat

              if [ -d "workflow/$(params.workflowId)" ]; then
                cp -r workflow/$(params.workflowId)/src/main/resources flat/$(params.workflowId)
                cp workflow/$(params.workflowId)/LICENSE flat/$(params.workflowId)
              else
                cp -r workflow/src/main/resources flat/$(params.workflowId)
                cp workflow/LICENSE flat/$(params.workflowId)
              fi

              if [ "$(params.convertToFlat)" == "false" ]; then
                rm -rf workflow/src/main/resources
                mv workflow/src flat/$(params.workflowId)/
              fi

              ls flat/$(params.workflowId)

              curl -L https://raw.githubusercontent.com/rhdhorchestrator/serverless-workflows/main/pipeline/workflow-builder.Dockerfile -o flat/workflow-builder.Dockerfile
      ---
      apiVersion: tekton.dev/v1
      kind: Task
      metadata:
        name: build-manifests
        namespace: {{backstage-ns}}
      spec:
        workspaces:
          - name: workflow-source
        params:
          - name: workflowId
            description: The workflow ID from the repository
            type: string
        steps:
          - name: build-manifests
            image: registry.access.redhat.com/ubi9-minimal
            workingDir: $(workspaces.workflow-source.path)/flat/$(params.workflowId)
            script: |
              microdnf install -y tar gzip
              KN_CLI_URL="https://developers.redhat.com/content-gateway/file/pub/cgw/serverless-logic/1.35.0/kn-workflow-linux-amd64.tar.gz"
              curl -L "$KN_CLI_URL" | tar -xz --no-same-owner && chmod +x kn-workflow-linux-amd64 && mv kn-workflow-linux-amd64 kn-workflow
              ./kn-workflow gen-manifest --namespace ""
      ---
      apiVersion: tekton.dev/v1
      kind: Task
      metadata:
        name: build-gitops
        namespace: {{backstage-ns}}
      spec:
        workspaces:
          - name: workflow-source
          - name: workflow-gitops
        params:
          - name: workflowId
            description: The workflow ID from the repository
            type: string
          - name: imageTag
            type: string
        steps:
          - name: build-gitops
            image: registry.access.redhat.com/ubi9-minimal
            workingDir: $(workspaces.workflow-gitops.path)/workflow-gitops
            script: |
              cp $(workspaces.workflow-source.path)/flat/$(params.workflowId)/manifests/* kustomize/base
              microdnf install -y findutils && microdnf clean all
              cd kustomize
              ./updater.sh $(params.workflowId) $(params.imageTag)
      ---
      apiVersion: tekton.dev/v1
      kind: Pipeline
      metadata:
        name: workflow-deployment
        namespace: {{backstage-ns}}
      spec:
        description: |
          This pipeline clones a git repo, builds a Docker image with Kaniko and
          pushes it to a registry
        params:
          - name: gitUrl
            description: The SSH URL of the repository to clone
            type: string
          - name: gitOpsUrl
            description: The SSH URL of the config repository for pushing the changes
            type: string
          - name: workflowId
            description: The workflow ID from the repository
            type: string
          - name: convertToFlat
            description: Whether conversion to flat layout is needed or it's already flattened
            type: string
            default: "true"
          - name: quayOrgName
            description: The Quay Organization Name of the published workflow
            type: string
          - name: quayRepoName
            description: The Quay Repository Name of the published workflow
            type: string
        workspaces:
          - name: workflow-source
          - name: workflow-gitops
          - name: ssh-creds
          - name: docker-credentials
        tasks:
          - name: fetch-workflow
            taskRef:
              name: git-cli
            workspaces:
              - name: source
                workspace: workflow-source
              - name: ssh-directory
                workspace: ssh-creds
            params:
              - name: GIT_USER_NAME
                value: The Orchestrator Tekton Pipeline
              - name: GIT_USER_EMAIL
                value: rhdhorchestrator@redhat.com
              - name: USER_HOME
                value: /home/git
              - name: GIT_SCRIPT
                value: |
                  eval "$(ssh-agent -s)"
                  ssh-add "${PARAM_USER_HOME}"/.ssh/id_rsa
                  GIT_SSH_COMMAND="ssh -o UserKnownHostsFile=${PARAM_USER_HOME}/.ssh/known_hosts" git clone $(params.gitUrl) workflow
                  cd workflow
          - name: fetch-workflow-gitops
            taskRef:
              name: git-cli
            workspaces:
              - name: source
                workspace: workflow-gitops
              - name: ssh-directory
                workspace: ssh-creds
            params:
              - name: GIT_USER_NAME
                value: The Orchestrator Tekton Pipeline
              - name: GIT_USER_EMAIL
                value: rhdhorchestrator@redhat.com
              - name: USER_HOME
                value: /home/git
              - name: GIT_SCRIPT
                value: |
                  eval "$(ssh-agent -s)"
                  ssh-add "${PARAM_USER_HOME}"/.ssh/id_rsa
                  GIT_SSH_COMMAND="ssh -o UserKnownHostsFile=${PARAM_USER_HOME}/.ssh/known_hosts" git clone $(params.gitOpsUrl) workflow-gitops
          - name: flatten-workflow
            runAfter: ["fetch-workflow"]
            taskRef:
              name: flattener
            workspaces:
              - name: workflow-source
                workspace: workflow-source
            params:
              - name: workflowId
                value: $(params.workflowId)
              - name: convertToFlat
                value: $(params.convertToFlat)
          - name: build-manifests
            runAfter: ["flatten-workflow"]
            taskRef:
              name: build-manifests
            workspaces:
              - name: workflow-source
                workspace: workflow-source
            params:
              - name: workflowId
                value: $(params.workflowId)
          - name: build-gitops
            runAfter: ["build-manifests", "fetch-workflow-gitops"]
            taskRef:
              name: build-gitops
            workspaces:
              - name: workflow-source
                workspace: workflow-source
              - name: workflow-gitops
                workspace: workflow-gitops
            params:
              - name: workflowId
                value: $(params.workflowId)
              - name: imageTag
                value: $(tasks.fetch-workflow.results.commit)
          - name: build-and-push-image
            runAfter: ["flatten-workflow"]
            taskRef:
              resolver: cluster
              params:
              - name: kind
                value: task
              - name: name
                value: buildah
              - name: namespace
                value: openshift-pipelines
            workspaces:
              - name: source
                workspace: workflow-source
              - name: dockerconfig
                workspace: docker-credentials
            params:
              - name: IMAGE
                value: quay.io/$(params.quayOrgName)/$(params.quayRepoName):$(tasks.fetch-workflow.results.commit)
              - name: DOCKERFILE
                value: flat/workflow-builder.Dockerfile
              - name: CONTEXT
                value: flat/$(params.workflowId)
              - name: BUILD_EXTRA_ARGS
                value: '--authfile=/workspace/dockerconfig/.dockerconfigjson --ulimit nofile=4096:4096 --build-arg WF_RESOURCES=. '
          - name: push-workflow-gitops
            runAfter: ["build-gitops", "build-and-push-image"]
            taskRef:
              name: git-cli
            workspaces:
              - name: source
                workspace: workflow-gitops
              - name: ssh-directory
                workspace: ssh-creds
            params:
              - name: GIT_USER_NAME
                value: The Orchestrator Tekton Pipeline
              - name: GIT_USER_EMAIL
                value: rhdhorchestrator@redhat.com
              - name: USER_HOME
                value: /home/git
              - name: GIT_SCRIPT
                value: |
                  WORKFLOW_COMMIT=$(tasks.fetch-workflow.results.commit)

                  eval "$(ssh-agent -s)"
                  ssh-add "${PARAM_USER_HOME}"/.ssh/id_rsa

                  cd workflow-gitops
                  git add .
                  git diff
                  # TODO: create PR
                  git commit -m "Deployment for workflow commit $WORKFLOW_COMMIT from $(params.gitUrl)"
                  # TODO: parametrize branch
                  GIT_SSH_COMMAND="ssh -o UserKnownHostsFile=${PARAM_USER_HOME}/.ssh/known_hosts" git push origin main
  kind: ConfigMap
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","data":{"argocd.yaml":"---\napiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: argocd-app-project\nspec:\n  destinations:\n  - name: '*'\n    namespace: '*'\n    server: '*'\n  sourceRepos:\n  - '*'","sonataflow.yaml":"apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-infra-ns-to-workflow-ns # hardcoded\nspec:\n  podSelector: {}\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              # Allow knative events to be delivered to workflows.\n              kubernetes.io/metadata.name: knative-eventing\n        - namespaceSelector:\n            matchLabels:\n              # Allow auxiliary knative function for workflow (such as m2k-save-transformation)\n              kubernetes.io/metadata.name: knative-serving\n        - namespaceSelector:\n            matchLabels:\n              # Allow openshift serverless logic operator controller pod to access all pods in sonataflow\n              kubernetes.io/metadata.name: openshift-serverless-logic\n---\n# NetworkPolicy to unblock incoming traffic to the namespace\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-external-communication # hardcoded\nspec:\n  podSelector: {}\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              # Allow knative events to be delivered to workflows.\n              policy-group.network.openshift.io/ingress: \"\"\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-intra-network # hardcoded\nspec:\n  # Apply this policy to all pods in the namespace\n  podSelector: {}\n  # Specify policy type as 'Ingress' to control incoming traffic rules\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n        # Allow ingress from any pod within the same namespace\n        - podSelector: {}\n---\n# NetworkPolicy to allow openshift-user-workload-monitoring pods to access all pods within the workflow's namespace\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-monitoring-to-sonataflow-and-workflows # hardcoded\nspec:\n  # Apply this policy to all pods in the namespace\n  podSelector: {}\n  # Specify policy type as 'Ingress' to control incoming traffic rules\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              # Allow openshift-user-workload-monitoring pods to access the workflow.\n              kubernetes.io/metadata.name: openshift-user-workload-monitoring\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: create-sonataflow-database-{{backstage-name}}\nspec:\n  ttlSecondsAfterFinished: 30\n  activeDeadlineSeconds: 120\n  template:\n    spec:\n      containers:\n        - name: psql\n          image: quay.io/fedora/postgresql-15:latest\n          resources:\n            limits:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n            requests:\n              cpu: \"100m\"\n              memory: \"64Mi\"\n          securityContext:\n            readOnlyRootFilesystem: true\n            allowPrivilegeEscalation: false\n            runAsNonRoot: true\n            capabilities:\n              drop:\n                - ALL\n          envFrom:\n            - secretRef:\n                name: backstage-psql-secret-{{backstage-name}}\n          command: [ \"sh\", \"-c\" ]\n          args:\n            - |\n              set -e\n              DB_EXISTS=$(PGPASSWORD=${POSTGRES_PASSWORD} psql -h ${POSTGRES_HOST} -p ${POSTGRES_PORT} -U ${POSTGRES_USER} -tAc \"SELECT 1 FROM pg_database WHERE datname='backstage_plugin_orchestrator'\" postgres)\n              if [ -z \"$DB_EXISTS\" ]; then\n                PGPASSWORD=${POSTGRES_PASSWORD} psql -h ${POSTGRES_HOST} -p ${POSTGRES_PORT} -U ${POSTGRES_USER} -c \"CREATE DATABASE backstage_plugin_orchestrator;\" postgres\n              fi\n      restartPolicy: Never\n---\napiVersion: sonataflow.org/v1alpha08\nkind: SonataFlowPlatform\nmetadata:\n  name: sonataflow-platform\nspec:\n  monitoring:\n    enabled: true\n  services:\n    dataIndex:\n      enabled: true\n      persistence:\n        postgresql:\n          secretRef:\n            name: backstage-psql-secret-{{backstage-name}}\n            userKey: POSTGRES_USER\n            passwordKey: POSTGRES_PASSWORD\n          serviceRef:\n            name: backstage-psql-{{backstage-name}}\n            namespace: {{backstage-ns}}\n            databaseName: backstage_plugin_orchestrator\n    jobService:\n      enabled: true\n      persistence:\n        postgresql:\n          secretRef:\n            name: backstage-psql-secret-{{backstage-name}}\n            userKey: POSTGRES_USER\n            passwordKey: POSTGRES_PASSWORD\n          serviceRef:\n            name: backstage-psql-{{backstage-name}}\n            namespace: {{backstage-ns}}\n            databaseName: backstage_plugin_orchestrator\n","tekton.yaml":"---\napiVersion: tekton.dev/v1\nkind: Task\nmetadata:\n  name: git-cli\n  namespace: {{backstage-ns}}\n  labels:\n    app.kubernetes.io/version: \"0.4\"\n  annotations:\n    tekton.dev/pipelines.minVersion: \"0.21.0\"\n    tekton.dev/categories: Git\n    tekton.dev/tags: git\n    tekton.dev/displayName: \"git cli\"\n    tekton.dev/platforms: \"linux/amd64,linux/s390x,linux/ppc64le\"\nspec:\n  description: \u003e-\n    This task can be used to perform git operations.\n\n    Git command that needs to be run can be passed as a script to\n    the task. This task needs authentication to git in order to push\n    after the git operation.\n\n  workspaces:\n    - name: source\n      description: A workspace that contains the fetched git repository.\n\n    - name: input\n      optional: true\n      description: |\n        An optional workspace that contains the files that need to be added to git. You can\n        access the workspace from your script using `$(workspaces.input.path)`, for instance:\n\n        cp $(workspaces.input.path)/file_that_i_want .\n        git add file_that_i_want\n        # etc\n\n    - name: ssh-directory\n      optional: true\n      description: |\n        A .ssh directory with private key, known_hosts, config, etc. Copied to\n        the user's home before git commands are executed. Used to authenticate\n        with the git remote when performing the clone. Binding a Secret to this\n        Workspace is strongly recommended over other volume types.\n\n    - name: basic-auth\n      optional: true\n      description: |\n        A Workspace containing a .gitconfig and .git-credentials file. These\n        will be copied to the user's home before any git commands are run. Any\n        other files in this Workspace are ignored. It is strongly recommended\n        to use ssh-directory over basic-auth whenever possible and to bind a\n        Secret to this Workspace over other volume types.\n  params:\n    - name: BASE_IMAGE\n      description: |\n        The base image for the task.\n      type: string\n      # TODO: Deprecate use of root image.\n      default: cgr.dev/chainguard/git:root-2.39@sha256:7759f87050dd8bacabe61354d75ccd7f864d6b6f8ec42697db7159eccd491139\n\n    - name: GIT_USER_NAME\n      type: string\n      description: |\n        Git user name for performing git operation.\n      default: \"\"\n\n    - name: GIT_USER_EMAIL\n      type: string\n      description: |\n        Git user email for performing git operation.\n      default: \"\"\n\n    - name: GIT_SCRIPT\n      description: The git script to run.\n      type: string\n      default: |\n        git help\n\n    - name: USER_HOME\n      description: |\n        Absolute path to the user's home directory. Set this explicitly if you are running the image as a non-root user or have overridden\n        the gitInitImage param with an image containing custom user configuration.\n      type: string\n      default: \"/root\"\n\n    - name: VERBOSE\n      description: Log the commands that are executed during `git-clone`'s operation.\n      type: string\n      default: \"true\"\n\n  results:\n    - name: commit\n      description: The precise commit SHA after the git operation.\n\n  steps:\n    - name: git\n      image: $(params.BASE_IMAGE)\n      workingDir: $(workspaces.source.path)\n      env:\n      - name: HOME\n        value: $(params.USER_HOME)\n      - name: PARAM_VERBOSE\n        value: $(params.VERBOSE)\n      - name: PARAM_USER_HOME\n        value: $(params.USER_HOME)\n      - name: WORKSPACE_OUTPUT_PATH\n        value: $(workspaces.output.path)\n      - name: WORKSPACE_SSH_DIRECTORY_BOUND\n        value: $(workspaces.ssh-directory.bound)\n      - name: WORKSPACE_SSH_DIRECTORY_PATH\n        value: $(workspaces.ssh-directory.path)\n      - name: WORKSPACE_BASIC_AUTH_DIRECTORY_BOUND\n        value: $(workspaces.basic-auth.bound)\n      - name: WORKSPACE_BASIC_AUTH_DIRECTORY_PATH\n        value: $(workspaces.basic-auth.path)\n      script: |\n        #!/usr/bin/env sh\n        set -eu\n\n        if [ \"${PARAM_VERBOSE}\" = \"true\" ] ; then\n          set -x\n        fi\n\n        if [ \"${WORKSPACE_BASIC_AUTH_DIRECTORY_BOUND}\" = \"true\" ] ; then\n          cp \"${WORKSPACE_BASIC_AUTH_DIRECTORY_PATH}/.git-credentials\" \"${PARAM_USER_HOME}/.git-credentials\"\n          cp \"${WORKSPACE_BASIC_AUTH_DIRECTORY_PATH}/.gitconfig\" \"${PARAM_USER_HOME}/.gitconfig\"\n          chmod 400 \"${PARAM_USER_HOME}/.git-credentials\"\n          chmod 400 \"${PARAM_USER_HOME}/.gitconfig\"\n        fi\n\n        if [ \"${WORKSPACE_SSH_DIRECTORY_BOUND}\" = \"true\" ] ; then\n          cp -R \"${WORKSPACE_SSH_DIRECTORY_PATH}\" \"${PARAM_USER_HOME}\"/.ssh\n          chmod 700 \"${PARAM_USER_HOME}\"/.ssh\n          chmod -R 400 \"${PARAM_USER_HOME}\"/.ssh/*\n        fi\n\n        # Setting up the config for the git.\n        git config --global user.email \"$(params.GIT_USER_EMAIL)\"\n        git config --global user.name \"$(params.GIT_USER_NAME)\"\n\n        eval '$(params.GIT_SCRIPT)'\n\n        RESULT_SHA=\"$(git rev-parse HEAD | tr -d '\\n')\"\n        EXIT_CODE=\"$?\"\n        if [ \"$EXIT_CODE\" != 0 ]\n        then\n          exit $EXIT_CODE\n        fi\n        # Make sure we don't add a trailing newline to the result!\n        printf \"%s\" \"$RESULT_SHA\" \u003e \"$(results.commit.path)\"\n      # Patch to apply on OpenShift\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65532\n---\napiVersion: tekton.dev/v1\nkind: Task\nmetadata:\n  name: flattener\n  namespace: {{backstage-ns}}\nspec:\n  workspaces:\n    - name: workflow-source\n  params:\n    - name: workflowId\n      description: The workflow ID from the repository\n      type: string\n    - name: convertToFlat\n      description: Whether conversion to flat layout is needed or it's already flattened\n      type: string\n      default: \"true\"\n  steps:\n    - name: flatten\n      image: registry.access.redhat.com/ubi9-minimal\n      workingDir: $(workspaces.workflow-source.path)\n      script: |\n        ROOT=/workspace/workflow\n        TARGET=flat\n        mkdir -p flat\n\n        if [ -d \"workflow/$(params.workflowId)\" ]; then\n          cp -r workflow/$(params.workflowId)/src/main/resources flat/$(params.workflowId)\n          cp workflow/$(params.workflowId)/LICENSE flat/$(params.workflowId)\n        else\n          cp -r workflow/src/main/resources flat/$(params.workflowId)\n          cp workflow/LICENSE flat/$(params.workflowId)\n        fi\n\n        if [ \"$(params.convertToFlat)\" == \"false\" ]; then\n          rm -rf workflow/src/main/resources\n          mv workflow/src flat/$(params.workflowId)/\n        fi\n\n        ls flat/$(params.workflowId)\n\n        curl -L https://raw.githubusercontent.com/rhdhorchestrator/serverless-workflows/main/pipeline/workflow-builder.Dockerfile -o flat/workflow-builder.Dockerfile\n---\napiVersion: tekton.dev/v1\nkind: Task\nmetadata:\n  name: build-manifests\n  namespace: {{backstage-ns}}\nspec:\n  workspaces:\n    - name: workflow-source\n  params:\n    - name: workflowId\n      description: The workflow ID from the repository\n      type: string\n  steps:\n    - name: build-manifests\n      image: registry.access.redhat.com/ubi9-minimal\n      workingDir: $(workspaces.workflow-source.path)/flat/$(params.workflowId)\n      script: |\n        microdnf install -y tar gzip\n        KN_CLI_URL=\"https://developers.redhat.com/content-gateway/file/pub/cgw/serverless-logic/1.35.0/kn-workflow-linux-amd64.tar.gz\"\n        curl -L \"$KN_CLI_URL\" | tar -xz --no-same-owner \u0026\u0026 chmod +x kn-workflow-linux-amd64 \u0026\u0026 mv kn-workflow-linux-amd64 kn-workflow\n        ./kn-workflow gen-manifest --namespace \"\"\n---\napiVersion: tekton.dev/v1\nkind: Task\nmetadata:\n  name: build-gitops\n  namespace: {{backstage-ns}}\nspec:\n  workspaces:\n    - name: workflow-source\n    - name: workflow-gitops\n  params:\n    - name: workflowId\n      description: The workflow ID from the repository\n      type: string\n    - name: imageTag\n      type: string\n  steps:\n    - name: build-gitops\n      image: registry.access.redhat.com/ubi9-minimal\n      workingDir: $(workspaces.workflow-gitops.path)/workflow-gitops\n      script: |\n        cp $(workspaces.workflow-source.path)/flat/$(params.workflowId)/manifests/* kustomize/base\n        microdnf install -y findutils \u0026\u0026 microdnf clean all\n        cd kustomize\n        ./updater.sh $(params.workflowId) $(params.imageTag)\n---\napiVersion: tekton.dev/v1\nkind: Pipeline\nmetadata:\n  name: workflow-deployment\n  namespace: {{backstage-ns}}\nspec:\n  description: |\n    This pipeline clones a git repo, builds a Docker image with Kaniko and\n    pushes it to a registry\n  params:\n    - name: gitUrl\n      description: The SSH URL of the repository to clone\n      type: string\n    - name: gitOpsUrl\n      description: The SSH URL of the config repository for pushing the changes\n      type: string\n    - name: workflowId\n      description: The workflow ID from the repository\n      type: string\n    - name: convertToFlat\n      description: Whether conversion to flat layout is needed or it's already flattened\n      type: string\n      default: \"true\"\n    - name: quayOrgName\n      description: The Quay Organization Name of the published workflow\n      type: string\n    - name: quayRepoName\n      description: The Quay Repository Name of the published workflow\n      type: string\n  workspaces:\n    - name: workflow-source\n    - name: workflow-gitops\n    - name: ssh-creds\n    - name: docker-credentials\n  tasks:\n    - name: fetch-workflow\n      taskRef:\n        name: git-cli\n      workspaces:\n        - name: source\n          workspace: workflow-source\n        - name: ssh-directory\n          workspace: ssh-creds\n      params:\n        - name: GIT_USER_NAME\n          value: The Orchestrator Tekton Pipeline\n        - name: GIT_USER_EMAIL\n          value: rhdhorchestrator@redhat.com\n        - name: USER_HOME\n          value: /home/git\n        - name: GIT_SCRIPT\n          value: |\n            eval \"$(ssh-agent -s)\"\n            ssh-add \"${PARAM_USER_HOME}\"/.ssh/id_rsa\n            GIT_SSH_COMMAND=\"ssh -o UserKnownHostsFile=${PARAM_USER_HOME}/.ssh/known_hosts\" git clone $(params.gitUrl) workflow\n            cd workflow\n    - name: fetch-workflow-gitops\n      taskRef:\n        name: git-cli\n      workspaces:\n        - name: source\n          workspace: workflow-gitops\n        - name: ssh-directory\n          workspace: ssh-creds\n      params:\n        - name: GIT_USER_NAME\n          value: The Orchestrator Tekton Pipeline\n        - name: GIT_USER_EMAIL\n          value: rhdhorchestrator@redhat.com\n        - name: USER_HOME\n          value: /home/git\n        - name: GIT_SCRIPT\n          value: |\n            eval \"$(ssh-agent -s)\"\n            ssh-add \"${PARAM_USER_HOME}\"/.ssh/id_rsa\n            GIT_SSH_COMMAND=\"ssh -o UserKnownHostsFile=${PARAM_USER_HOME}/.ssh/known_hosts\" git clone $(params.gitOpsUrl) workflow-gitops\n    - name: flatten-workflow\n      runAfter: [\"fetch-workflow\"]\n      taskRef:\n        name: flattener\n      workspaces:\n        - name: workflow-source\n          workspace: workflow-source\n      params:\n        - name: workflowId\n          value: $(params.workflowId)\n        - name: convertToFlat\n          value: $(params.convertToFlat)\n    - name: build-manifests\n      runAfter: [\"flatten-workflow\"]\n      taskRef:\n        name: build-manifests\n      workspaces:\n        - name: workflow-source\n          workspace: workflow-source\n      params:\n        - name: workflowId\n          value: $(params.workflowId)\n    - name: build-gitops\n      runAfter: [\"build-manifests\", \"fetch-workflow-gitops\"]\n      taskRef:\n        name: build-gitops\n      workspaces:\n        - name: workflow-source\n          workspace: workflow-source\n        - name: workflow-gitops\n          workspace: workflow-gitops\n      params:\n        - name: workflowId\n          value: $(params.workflowId)\n        - name: imageTag\n          value: $(tasks.fetch-workflow.results.commit)\n    - name: build-and-push-image\n      runAfter: [\"flatten-workflow\"]\n      taskRef:\n        resolver: cluster\n        params:\n        - name: kind\n          value: task\n        - name: name\n          value: buildah\n        - name: namespace\n          value: openshift-pipelines\n      workspaces:\n        - name: source\n          workspace: workflow-source\n        - name: dockerconfig\n          workspace: docker-credentials\n      params:\n        - name: IMAGE\n          value: quay.io/$(params.quayOrgName)/$(params.quayRepoName):$(tasks.fetch-workflow.results.commit)\n        - name: DOCKERFILE\n          value: flat/workflow-builder.Dockerfile\n        - name: CONTEXT\n          value: flat/$(params.workflowId)\n        - name: BUILD_EXTRA_ARGS\n          value: '--authfile=/workspace/dockerconfig/.dockerconfigjson --ulimit nofile=4096:4096 --build-arg WF_RESOURCES=. '\n    - name: push-workflow-gitops\n      runAfter: [\"build-gitops\", \"build-and-push-image\"]\n      taskRef:\n        name: git-cli\n      workspaces:\n        - name: source\n          workspace: workflow-gitops\n        - name: ssh-directory\n          workspace: ssh-creds\n      params:\n        - name: GIT_USER_NAME\n          value: The Orchestrator Tekton Pipeline\n        - name: GIT_USER_EMAIL\n          value: rhdhorchestrator@redhat.com\n        - name: USER_HOME\n          value: /home/git\n        - name: GIT_SCRIPT\n          value: |\n            WORKFLOW_COMMIT=$(tasks.fetch-workflow.results.commit)\n\n            eval \"$(ssh-agent -s)\"\n            ssh-add \"${PARAM_USER_HOME}\"/.ssh/id_rsa\n\n            cd workflow-gitops\n            git add .\n            git diff\n            # TODO: create PR\n            git commit -m \"Deployment for workflow commit $WORKFLOW_COMMIT from $(params.gitUrl)\"\n            # TODO: parametrize branch\n            GIT_SSH_COMMAND=\"ssh -o UserKnownHostsFile=${PARAM_USER_HOME}/.ssh/known_hosts\" git push origin main"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"rhdh-plugin-deps","namespace":"rhdh-operator"}}
    creationTimestamp: "2025-10-23T09:43:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          .: {}
          f:argocd.yaml: {}
          f:sonataflow.yaml: {}
          f:tekton.yaml: {}
        f:metadata:
          f:annotations:
            .: {}
            f:kubectl.kubernetes.io/last-applied-configuration: {}
      manager: kubectl-client-side-apply
      operation: Update
      time: "2025-10-23T09:43:13Z"
    name: rhdh-plugin-deps
    namespace: rhdh-operator
    resourceVersion: "1217"
    uid: ba8027dc-12a5-4f73-9bdf-8c45ab230f59
kind: ConfigMapList
metadata:
  resourceVersion: "4661"
